{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "999525de",
   "metadata": {},
   "source": [
    "## Imports & Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddf2d534",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7e3afb71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Project root: /Users/alex/Desktop/TrustWorthyXAILocaltoGlobal\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root)\n",
    "pd.set_option('display.max_columns', 100)  # or however many you want\n",
    "pd.set_option('display.max_rows', 200)\n",
    "print(\"Project root:\", project_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9718c973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/alex/Desktop/TrustWorthyXAILocaltoGlobal/datasets/500ktest_dataset_lar.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/ymr48tf963d9_3y2jfmn5wfw0000gn/T/ipykernel_81901/349837994.py:6: DtypeWarning: Columns (23,24,25,27,28,29,30,31,32,37,42) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df=pd.read_csv(data_path)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>activity_year</th>\n",
       "      <th>lei</th>\n",
       "      <th>derived_msa_md</th>\n",
       "      <th>state_code</th>\n",
       "      <th>county_code</th>\n",
       "      <th>census_tract</th>\n",
       "      <th>conforming_loan_limit</th>\n",
       "      <th>derived_loan_product_type</th>\n",
       "      <th>derived_dwelling_category</th>\n",
       "      <th>derived_ethnicity</th>\n",
       "      <th>derived_race</th>\n",
       "      <th>derived_sex</th>\n",
       "      <th>action_taken</th>\n",
       "      <th>purchaser_type</th>\n",
       "      <th>preapproval</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>lien_status</th>\n",
       "      <th>reverse_mortgage</th>\n",
       "      <th>open_end_line_of_credit</th>\n",
       "      <th>business_or_commercial_purpose</th>\n",
       "      <th>loan_amount</th>\n",
       "      <th>combined_loan_to_value_ratio</th>\n",
       "      <th>interest_rate</th>\n",
       "      <th>rate_spread</th>\n",
       "      <th>hoepa_status</th>\n",
       "      <th>total_loan_costs</th>\n",
       "      <th>origination_charges</th>\n",
       "      <th>discount_points</th>\n",
       "      <th>lender_credits</th>\n",
       "      <th>loan_term</th>\n",
       "      <th>intro_rate_period</th>\n",
       "      <th>negative_amortization</th>\n",
       "      <th>interest_only_payment</th>\n",
       "      <th>balloon_payment</th>\n",
       "      <th>other_nonamortizing_features</th>\n",
       "      <th>property_value</th>\n",
       "      <th>construction_method</th>\n",
       "      <th>occupancy_type</th>\n",
       "      <th>manufactured_home_secured_property_type</th>\n",
       "      <th>manufactured_home_land_property_interest</th>\n",
       "      <th>total_units</th>\n",
       "      <th>income</th>\n",
       "      <th>debt_to_income_ratio</th>\n",
       "      <th>applicant_credit_score_type</th>\n",
       "      <th>co_applicant_credit_score_type</th>\n",
       "      <th>applicant_ethnicity_1</th>\n",
       "      <th>co_applicant_ethnicity_1</th>\n",
       "      <th>applicant_ethnicity_observed</th>\n",
       "      <th>co_applicant_ethnicity_observed</th>\n",
       "      <th>applicant_race_1</th>\n",
       "      <th>co_applicant_race_1</th>\n",
       "      <th>applicant_race_observed</th>\n",
       "      <th>co_applicant_race_observed</th>\n",
       "      <th>applicant_sex</th>\n",
       "      <th>co_applicant_sex</th>\n",
       "      <th>applicant_sex_observed</th>\n",
       "      <th>co_applicant_sex_observed</th>\n",
       "      <th>applicant_age</th>\n",
       "      <th>co_applicant_age</th>\n",
       "      <th>applicant_age_above_62</th>\n",
       "      <th>co_applicant_age_above_62</th>\n",
       "      <th>submission_of_application</th>\n",
       "      <th>initially_payable_to_institution</th>\n",
       "      <th>aus_1</th>\n",
       "      <th>denial_reason_1</th>\n",
       "      <th>tract_population</th>\n",
       "      <th>tract_minority_population_percent</th>\n",
       "      <th>ffiec_msa_md_median_family_income</th>\n",
       "      <th>tract_to_msa_income_percentage</th>\n",
       "      <th>tract_owner_occupied_units</th>\n",
       "      <th>tract_one_to_four_family_homes</th>\n",
       "      <th>tract_median_age_of_housing_units</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>98395</td>\n",
       "      <td>2024</td>\n",
       "      <td>549300ZX4OGRPOOEH505</td>\n",
       "      <td>28140</td>\n",
       "      <td>KS</td>\n",
       "      <td>20209.0</td>\n",
       "      <td>2.020904e+10</td>\n",
       "      <td>C</td>\n",
       "      <td>Conventional:First Lien</td>\n",
       "      <td>Single Family (1-4 Units):Site-Built</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>31</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>125000</td>\n",
       "      <td>90.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>-4.925</td>\n",
       "      <td>2</td>\n",
       "      <td>984.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>135000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>25-34</td>\n",
       "      <td>9999</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>1125</td>\n",
       "      <td>77.69</td>\n",
       "      <td>103000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>197</td>\n",
       "      <td>398</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>55868</td>\n",
       "      <td>2024</td>\n",
       "      <td>5493000NYUJT9UC6G261</td>\n",
       "      <td>45294</td>\n",
       "      <td>FL</td>\n",
       "      <td>12101.0</td>\n",
       "      <td>1.210103e+10</td>\n",
       "      <td>C</td>\n",
       "      <td>Conventional:First Lien</td>\n",
       "      <td>Single Family (1-4 Units):Site-Built</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>205000</td>\n",
       "      <td>80.0</td>\n",
       "      <td>7.49</td>\n",
       "      <td>0.947</td>\n",
       "      <td>2</td>\n",
       "      <td>7515.0</td>\n",
       "      <td>4865.0</td>\n",
       "      <td>3120.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>265000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>76.0</td>\n",
       "      <td>30%-&lt;36%</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>25-34</td>\n",
       "      <td>9999</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5067</td>\n",
       "      <td>39.73</td>\n",
       "      <td>88800</td>\n",
       "      <td>155.0</td>\n",
       "      <td>1401</td>\n",
       "      <td>1824</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>90858</td>\n",
       "      <td>2024</td>\n",
       "      <td>5493006MW6O2CE88BD43</td>\n",
       "      <td>39820</td>\n",
       "      <td>CA</td>\n",
       "      <td>6089.0</td>\n",
       "      <td>6.089011e+09</td>\n",
       "      <td>C</td>\n",
       "      <td>Conventional:Subordinate Lien</td>\n",
       "      <td>Single Family (1-4 Units):Site-Built</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>50%-60%</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>65-74</td>\n",
       "      <td>9999</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3467</td>\n",
       "      <td>23.74</td>\n",
       "      <td>89100</td>\n",
       "      <td>139.0</td>\n",
       "      <td>948</td>\n",
       "      <td>1242</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12143</td>\n",
       "      <td>2024</td>\n",
       "      <td>549300157R33LDICP344</td>\n",
       "      <td>99999</td>\n",
       "      <td>OH</td>\n",
       "      <td>39021.0</td>\n",
       "      <td>3.902101e+10</td>\n",
       "      <td>C</td>\n",
       "      <td>FHA:First Lien</td>\n",
       "      <td>Single Family (1-4 Units):Site-Built</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>215000</td>\n",
       "      <td>96.50000</td>\n",
       "      <td>6.500</td>\n",
       "      <td>1.310</td>\n",
       "      <td>2</td>\n",
       "      <td>7630.81</td>\n",
       "      <td>2125.00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>527.76</td>\n",
       "      <td>360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>215000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>67.0</td>\n",
       "      <td>46</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>25-34</td>\n",
       "      <td>9999</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3714</td>\n",
       "      <td>16.32</td>\n",
       "      <td>81600</td>\n",
       "      <td>105.0</td>\n",
       "      <td>993</td>\n",
       "      <td>1474</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4785</td>\n",
       "      <td>2024</td>\n",
       "      <td>ZF85QS7OXKPBG52R7N18</td>\n",
       "      <td>33340</td>\n",
       "      <td>WI</td>\n",
       "      <td>55079.0</td>\n",
       "      <td>5.507902e+10</td>\n",
       "      <td>C</td>\n",
       "      <td>Conventional:First Lien</td>\n",
       "      <td>Single Family (1-4 Units):Site-Built</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>205000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>25-34</td>\n",
       "      <td>9999</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4145</td>\n",
       "      <td>22.46</td>\n",
       "      <td>102100</td>\n",
       "      <td>91.0</td>\n",
       "      <td>1337</td>\n",
       "      <td>1944</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2289</td>\n",
       "      <td>2024</td>\n",
       "      <td>ZF85QS7OXKPBG52R7N18</td>\n",
       "      <td>24580</td>\n",
       "      <td>WI</td>\n",
       "      <td>55009.0</td>\n",
       "      <td>5.500900e+10</td>\n",
       "      <td>C</td>\n",
       "      <td>Conventional:First Lien</td>\n",
       "      <td>Single Family (1-4 Units):Site-Built</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Joint</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>295000</td>\n",
       "      <td>97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>305000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>86.0</td>\n",
       "      <td>36</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>25-34</td>\n",
       "      <td>25-34</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>8553</td>\n",
       "      <td>31.35</td>\n",
       "      <td>96900</td>\n",
       "      <td>106.0</td>\n",
       "      <td>2394</td>\n",
       "      <td>2781</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>53185</td>\n",
       "      <td>2024</td>\n",
       "      <td>5493000NYUJT9UC6G261</td>\n",
       "      <td>16984</td>\n",
       "      <td>IL</td>\n",
       "      <td>17197.0</td>\n",
       "      <td>1.719788e+10</td>\n",
       "      <td>C</td>\n",
       "      <td>FHA:First Lien</td>\n",
       "      <td>Single Family (1-4 Units):Site-Built</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>215000</td>\n",
       "      <td>90.0</td>\n",
       "      <td>6.75</td>\n",
       "      <td>1.339</td>\n",
       "      <td>2</td>\n",
       "      <td>9835.5</td>\n",
       "      <td>1690.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>235000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>57.0</td>\n",
       "      <td>49</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>25-34</td>\n",
       "      <td>9999</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>4057</td>\n",
       "      <td>67.44</td>\n",
       "      <td>109800</td>\n",
       "      <td>64.0</td>\n",
       "      <td>652</td>\n",
       "      <td>1410</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>80948</td>\n",
       "      <td>2024</td>\n",
       "      <td>549300HH0V8LECLNPQ26</td>\n",
       "      <td>32780</td>\n",
       "      <td>OR</td>\n",
       "      <td>41029.0</td>\n",
       "      <td>4.102900e+10</td>\n",
       "      <td>C</td>\n",
       "      <td>Conventional:Subordinate Lien</td>\n",
       "      <td>Single Family (1-4 Units):Site-Built</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>105000</td>\n",
       "      <td>77.68200</td>\n",
       "      <td>8.490</td>\n",
       "      <td>0.350</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>465000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>93.0</td>\n",
       "      <td>39</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>55-64</td>\n",
       "      <td>9999</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>4099</td>\n",
       "      <td>15.81</td>\n",
       "      <td>87800</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1307</td>\n",
       "      <td>1996</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>57094</td>\n",
       "      <td>2024</td>\n",
       "      <td>5493000NYUJT9UC6G261</td>\n",
       "      <td>28140</td>\n",
       "      <td>MO</td>\n",
       "      <td>29095.0</td>\n",
       "      <td>2.909501e+10</td>\n",
       "      <td>C</td>\n",
       "      <td>Conventional:First Lien</td>\n",
       "      <td>Single Family (1-4 Units):Site-Built</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>325000</td>\n",
       "      <td>95.0</td>\n",
       "      <td>6.624</td>\n",
       "      <td>0.623</td>\n",
       "      <td>2</td>\n",
       "      <td>6707.0</td>\n",
       "      <td>4672.5</td>\n",
       "      <td>3277.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>360.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>345000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>182.0</td>\n",
       "      <td>20%-&lt;30%</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>55-64</td>\n",
       "      <td>9999</td>\n",
       "      <td>No</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>5143</td>\n",
       "      <td>19.27</td>\n",
       "      <td>103000</td>\n",
       "      <td>103.0</td>\n",
       "      <td>1710</td>\n",
       "      <td>1940</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34506</td>\n",
       "      <td>2024</td>\n",
       "      <td>549300T9P3WXNMXX0I34</td>\n",
       "      <td>33460</td>\n",
       "      <td>MN</td>\n",
       "      <td>27037.0</td>\n",
       "      <td>2.703706e+10</td>\n",
       "      <td>C</td>\n",
       "      <td>Conventional:First Lien</td>\n",
       "      <td>Single Family (1-4 Units):Site-Built</td>\n",
       "      <td>Not Hispanic or Latino</td>\n",
       "      <td>White</td>\n",
       "      <td>Joint</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>285000</td>\n",
       "      <td>90.0</td>\n",
       "      <td>7.5</td>\n",
       "      <td>0.62</td>\n",
       "      <td>2</td>\n",
       "      <td>4480.22</td>\n",
       "      <td>1855.72</td>\n",
       "      <td>405.72</td>\n",
       "      <td>30.0</td>\n",
       "      <td>360.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>100.0</td>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>&lt;25</td>\n",
       "      <td>&lt;25</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2502</td>\n",
       "      <td>11.43</td>\n",
       "      <td>123700</td>\n",
       "      <td>78.0</td>\n",
       "      <td>722</td>\n",
       "      <td>841</td>\n",
       "      <td>58</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  activity_year                   lei  derived_msa_md state_code  \\\n",
       "0       98395           2024  549300ZX4OGRPOOEH505           28140         KS   \n",
       "1       55868           2024  5493000NYUJT9UC6G261           45294         FL   \n",
       "2       90858           2024  5493006MW6O2CE88BD43           39820         CA   \n",
       "3       12143           2024  549300157R33LDICP344           99999         OH   \n",
       "4        4785           2024  ZF85QS7OXKPBG52R7N18           33340         WI   \n",
       "5        2289           2024  ZF85QS7OXKPBG52R7N18           24580         WI   \n",
       "6       53185           2024  5493000NYUJT9UC6G261           16984         IL   \n",
       "7       80948           2024  549300HH0V8LECLNPQ26           32780         OR   \n",
       "8       57094           2024  5493000NYUJT9UC6G261           28140         MO   \n",
       "9       34506           2024  549300T9P3WXNMXX0I34           33460         MN   \n",
       "\n",
       "   county_code  census_tract conforming_loan_limit  \\\n",
       "0      20209.0  2.020904e+10                     C   \n",
       "1      12101.0  1.210103e+10                     C   \n",
       "2       6089.0  6.089011e+09                     C   \n",
       "3      39021.0  3.902101e+10                     C   \n",
       "4      55079.0  5.507902e+10                     C   \n",
       "5      55009.0  5.500900e+10                     C   \n",
       "6      17197.0  1.719788e+10                     C   \n",
       "7      41029.0  4.102900e+10                     C   \n",
       "8      29095.0  2.909501e+10                     C   \n",
       "9      27037.0  2.703706e+10                     C   \n",
       "\n",
       "       derived_loan_product_type             derived_dwelling_category  \\\n",
       "0        Conventional:First Lien  Single Family (1-4 Units):Site-Built   \n",
       "1        Conventional:First Lien  Single Family (1-4 Units):Site-Built   \n",
       "2  Conventional:Subordinate Lien  Single Family (1-4 Units):Site-Built   \n",
       "3                 FHA:First Lien  Single Family (1-4 Units):Site-Built   \n",
       "4        Conventional:First Lien  Single Family (1-4 Units):Site-Built   \n",
       "5        Conventional:First Lien  Single Family (1-4 Units):Site-Built   \n",
       "6                 FHA:First Lien  Single Family (1-4 Units):Site-Built   \n",
       "7  Conventional:Subordinate Lien  Single Family (1-4 Units):Site-Built   \n",
       "8        Conventional:First Lien  Single Family (1-4 Units):Site-Built   \n",
       "9        Conventional:First Lien  Single Family (1-4 Units):Site-Built   \n",
       "\n",
       "        derived_ethnicity derived_race derived_sex  action_taken  \\\n",
       "0      Hispanic or Latino        White      Female             1   \n",
       "1  Not Hispanic or Latino        White        Male             1   \n",
       "2  Not Hispanic or Latino        White      Female             3   \n",
       "3  Not Hispanic or Latino        White      Female             1   \n",
       "4  Not Hispanic or Latino        White        Male             4   \n",
       "5      Hispanic or Latino        White       Joint             3   \n",
       "6      Hispanic or Latino        White        Male             1   \n",
       "7  Not Hispanic or Latino        White        Male             1   \n",
       "8  Not Hispanic or Latino        White        Male             1   \n",
       "9  Not Hispanic or Latino        White       Joint             1   \n",
       "\n",
       "   purchaser_type  preapproval  loan_type  loan_purpose  lien_status  \\\n",
       "0               0            2          1            31            1   \n",
       "1               1            2          1            32            1   \n",
       "2               0            2          1             2            2   \n",
       "3              71            2          2             1            1   \n",
       "4               0            2          1             1            1   \n",
       "5               0            2          1             1            1   \n",
       "6              71            2          2             1            1   \n",
       "7               0            2          1             2            2   \n",
       "8              71            2          1             1            1   \n",
       "9              71            2          1             1            1   \n",
       "\n",
       "   reverse_mortgage  open_end_line_of_credit  business_or_commercial_purpose  \\\n",
       "0                 2                        2                               2   \n",
       "1                 2                        2                               2   \n",
       "2                 2                        2                               2   \n",
       "3                 2                        2                               2   \n",
       "4                 2                        2                               2   \n",
       "5                 2                        2                               2   \n",
       "6                 2                        2                               2   \n",
       "7                 2                        1                               2   \n",
       "8                 2                        2                               2   \n",
       "9                 2                        2                               2   \n",
       "\n",
       "   loan_amount combined_loan_to_value_ratio interest_rate rate_spread  \\\n",
       "0       125000                         90.0          3.18      -4.925   \n",
       "1       205000                         80.0          7.49       0.947   \n",
       "2        25000                          NaN           NaN         NaN   \n",
       "3       215000                     96.50000         6.500       1.310   \n",
       "4       205000                          NaN           NaN         NaN   \n",
       "5       295000                           97           NaN         NaN   \n",
       "6       215000                         90.0          6.75       1.339   \n",
       "7       105000                     77.68200         8.490       0.350   \n",
       "8       325000                         95.0         6.624       0.623   \n",
       "9       285000                         90.0           7.5        0.62   \n",
       "\n",
       "   hoepa_status total_loan_costs origination_charges discount_points  \\\n",
       "0             2            984.0                 0.0             NaN   \n",
       "1             2           7515.0              4865.0          3120.0   \n",
       "2             3              NaN                 NaN             NaN   \n",
       "3             2          7630.81             2125.00             NaN   \n",
       "4             3              NaN                 NaN             NaN   \n",
       "5             3              NaN                 NaN             NaN   \n",
       "6             2           9835.5              1690.0             NaN   \n",
       "7             2              NaN                 NaN             NaN   \n",
       "8             2           6707.0              4672.5          3277.5   \n",
       "9             2          4480.22             1855.72          405.72   \n",
       "\n",
       "  lender_credits loan_term intro_rate_period  negative_amortization  \\\n",
       "0            NaN     360.0              12.0                      2   \n",
       "1            NaN     360.0               NaN                      2   \n",
       "2            NaN       NaN               NaN                      2   \n",
       "3         527.76       360               NaN                      2   \n",
       "4            NaN       360               NaN                      2   \n",
       "5            NaN       360               NaN                      2   \n",
       "6            NaN     360.0               NaN                      2   \n",
       "7            NaN       300               NaN                      2   \n",
       "8            NaN     360.0               NaN                      2   \n",
       "9           30.0     360.0               NaN                      2   \n",
       "\n",
       "   interest_only_payment  balloon_payment  other_nonamortizing_features  \\\n",
       "0                      2                2                             2   \n",
       "1                      2                2                             2   \n",
       "2                      2                2                             2   \n",
       "3                      2                2                             2   \n",
       "4                      2                2                             2   \n",
       "5                      2                2                             2   \n",
       "6                      2                2                             2   \n",
       "7                      1                2                             2   \n",
       "8                      2                2                             2   \n",
       "9                      2                2                             2   \n",
       "\n",
       "  property_value  construction_method  occupancy_type  \\\n",
       "0       135000.0                    1               1   \n",
       "1       265000.0                    1               1   \n",
       "2            NaN                    1               1   \n",
       "3         215000                    1               1   \n",
       "4            NaN                    1               1   \n",
       "5         305000                    1               1   \n",
       "6       235000.0                    1               1   \n",
       "7         465000                    1               1   \n",
       "8       345000.0                    1               1   \n",
       "9       325000.0                    1               1   \n",
       "\n",
       "   manufactured_home_secured_property_type  \\\n",
       "0                                        3   \n",
       "1                                        3   \n",
       "2                                        3   \n",
       "3                                        3   \n",
       "4                                        3   \n",
       "5                                        3   \n",
       "6                                        3   \n",
       "7                                        3   \n",
       "8                                        3   \n",
       "9                                        3   \n",
       "\n",
       "   manufactured_home_land_property_interest total_units  income  \\\n",
       "0                                         5           1     NaN   \n",
       "1                                         5           1    76.0   \n",
       "2                                         5           1    47.0   \n",
       "3                                         5           1    67.0   \n",
       "4                                         5           1    60.0   \n",
       "5                                         5           1    86.0   \n",
       "6                                         5           1    57.0   \n",
       "7                                         5           1    93.0   \n",
       "8                                         5           1   182.0   \n",
       "9                                         5           1   100.0   \n",
       "\n",
       "  debt_to_income_ratio  applicant_credit_score_type  \\\n",
       "0                   37                            2   \n",
       "1             30%-<36%                            3   \n",
       "2              50%-60%                            9   \n",
       "3                   46                            1   \n",
       "4                  NaN                            9   \n",
       "5                   36                            1   \n",
       "6                   49                            1   \n",
       "7                   39                           11   \n",
       "8             20%-<30%                            3   \n",
       "9                   45                            3   \n",
       "\n",
       "   co_applicant_credit_score_type  applicant_ethnicity_1  \\\n",
       "0                              10                    1.0   \n",
       "1                              10                    2.0   \n",
       "2                              10                    2.0   \n",
       "3                              10                    2.0   \n",
       "4                               9                    2.0   \n",
       "5                               9                    1.0   \n",
       "6                              10                    1.0   \n",
       "7                              10                    2.0   \n",
       "8                              10                    2.0   \n",
       "9                               9                    2.0   \n",
       "\n",
       "   co_applicant_ethnicity_1  applicant_ethnicity_observed  \\\n",
       "0                       5.0                             2   \n",
       "1                       5.0                             2   \n",
       "2                       5.0                             1   \n",
       "3                       5.0                             2   \n",
       "4                       5.0                             2   \n",
       "5                       1.0                             2   \n",
       "6                       5.0                             2   \n",
       "7                       5.0                             2   \n",
       "8                       5.0                             2   \n",
       "9                       2.0                             2   \n",
       "\n",
       "   co_applicant_ethnicity_observed  applicant_race_1  co_applicant_race_1  \\\n",
       "0                                4               5.0                  8.0   \n",
       "1                                4               5.0                  8.0   \n",
       "2                                4               5.0                  8.0   \n",
       "3                                4               5.0                  8.0   \n",
       "4                                4               5.0                  8.0   \n",
       "5                                2               5.0                  5.0   \n",
       "6                                4               5.0                  8.0   \n",
       "7                                4               5.0                  8.0   \n",
       "8                                4               5.0                  8.0   \n",
       "9                                2               5.0                  5.0   \n",
       "\n",
       "   applicant_race_observed  co_applicant_race_observed  applicant_sex  \\\n",
       "0                        2                           4              2   \n",
       "1                        2                           4              1   \n",
       "2                        1                           4              2   \n",
       "3                        2                           4              2   \n",
       "4                        2                           4              1   \n",
       "5                        2                           2              1   \n",
       "6                        2                           4              1   \n",
       "7                        2                           4              1   \n",
       "8                        2                           4              1   \n",
       "9                        2                           2              2   \n",
       "\n",
       "   co_applicant_sex  applicant_sex_observed  co_applicant_sex_observed  \\\n",
       "0                 5                       2                          4   \n",
       "1                 5                       2                          4   \n",
       "2                 5                       1                          4   \n",
       "3                 5                       2                          4   \n",
       "4                 5                       2                          4   \n",
       "5                 2                       2                          2   \n",
       "6                 5                       2                          4   \n",
       "7                 5                       2                          4   \n",
       "8                 5                       2                          4   \n",
       "9                 1                       2                          2   \n",
       "\n",
       "  applicant_age co_applicant_age applicant_age_above_62  \\\n",
       "0         25-34             9999                     No   \n",
       "1         25-34             9999                     No   \n",
       "2         65-74             9999                    Yes   \n",
       "3         25-34             9999                     No   \n",
       "4         25-34             9999                     No   \n",
       "5         25-34            25-34                     No   \n",
       "6         25-34             9999                     No   \n",
       "7         55-64             9999                     No   \n",
       "8         55-64             9999                     No   \n",
       "9           <25              <25                     No   \n",
       "\n",
       "  co_applicant_age_above_62  submission_of_application  \\\n",
       "0                       NaN                          1   \n",
       "1                       NaN                          1   \n",
       "2                       NaN                          1   \n",
       "3                       NaN                          1   \n",
       "4                       NaN                          1   \n",
       "5                        No                          1   \n",
       "6                       NaN                          1   \n",
       "7                       NaN                          1   \n",
       "8                       NaN                          1   \n",
       "9                        No                          1   \n",
       "\n",
       "   initially_payable_to_institution  aus_1  denial_reason_1  tract_population  \\\n",
       "0                                 1      6               10              1125   \n",
       "1                                 1      2               10              5067   \n",
       "2                                 1      6                1              3467   \n",
       "3                                 1      3               10              3714   \n",
       "4                                 1      1               10              4145   \n",
       "5                                 1      1                3              8553   \n",
       "6                                 1      1               10              4057   \n",
       "7                                 1      6               10              4099   \n",
       "8                                 1      1               10              5143   \n",
       "9                                 1      1               10              2502   \n",
       "\n",
       "   tract_minority_population_percent  ffiec_msa_md_median_family_income  \\\n",
       "0                              77.69                             103000   \n",
       "1                              39.73                              88800   \n",
       "2                              23.74                              89100   \n",
       "3                              16.32                              81600   \n",
       "4                              22.46                             102100   \n",
       "5                              31.35                              96900   \n",
       "6                              67.44                             109800   \n",
       "7                              15.81                              87800   \n",
       "8                              19.27                             103000   \n",
       "9                              11.43                             123700   \n",
       "\n",
       "   tract_to_msa_income_percentage  tract_owner_occupied_units  \\\n",
       "0                             0.0                         197   \n",
       "1                           155.0                        1401   \n",
       "2                           139.0                         948   \n",
       "3                           105.0                         993   \n",
       "4                            91.0                        1337   \n",
       "5                           106.0                        2394   \n",
       "6                            64.0                         652   \n",
       "7                           114.0                        1307   \n",
       "8                           103.0                        1710   \n",
       "9                            78.0                         722   \n",
       "\n",
       "   tract_one_to_four_family_homes  tract_median_age_of_housing_units  \n",
       "0                             398                                 43  \n",
       "1                            1824                                  0  \n",
       "2                            1242                                 34  \n",
       "3                            1474                                 59  \n",
       "4                            1944                                 74  \n",
       "5                            2781                                 43  \n",
       "6                            1410                                  0  \n",
       "7                            1996                                 25  \n",
       "8                            1940                                 43  \n",
       "9                             841                                 58  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename=\"500ktest_dataset_lar.csv\"\n",
    "script_dir = os.path.dirname('__file__')\n",
    "data_path = os.path.join(script_dir, '..', 'datasets', filename)\n",
    "data_path = os.path.abspath(data_path)\n",
    "print(data_path)\n",
    "df=pd.read_csv(data_path)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80049255",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7c704d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs: 0\n",
      "Target distribution:\n",
      "action_taken\n",
      "1    252377\n",
      "2     14605\n",
      "3     85576\n",
      "4     62995\n",
      "5     23941\n",
      "6     52115\n",
      "7      1963\n",
      "8      6317\n",
      "Name: count, dtype: int64\n",
      "First, we are going to drop all the columns where action_taken={4, 5, 6, 7, 8}. \n",
      "This leaves us with 352558 rows\n",
      "Combining categories 1 and 2 because they both indicate loan approval\n",
      "Binary target distribution:\n",
      "action_taken\n",
      "0     85576\n",
      "1    266982\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "target='action_taken'\n",
    "print(\"Number of NaNs:\", df[target].isna().sum())\n",
    "print(f\"Target distribution:\\n{df[target].value_counts().sort_index()}\")\n",
    "print(\"First, we are going to drop all the columns where action_taken={4, 5, 6, 7, 8}. \")\n",
    "df = df[~df[\"action_taken\"].isin([4,5,6,7,8])].copy()\n",
    "print(f\"This leaves us with {len(df)} rows\")\n",
    "#Next we combine categories 1 and 2 because they both indicate the loan was approved.\n",
    "print(\"Combining categories 1 and 2 because they both indicate loan approval\")\n",
    "df['action_taken']=df['action_taken'].isin([1,2]).astype(int)\n",
    "print(f\"Binary target distribution:\\n{df[target].value_counts().sort_index()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47454bb",
   "metadata": {},
   "source": [
    "### Unused/Optional Features\n",
    "1. Census features are optional because they are appended to the public HMDA data after collection, and thus are not used in initial loan approval\n",
    "2. Irrelevant features are dropped because they consist of categories such as AUS1 (the automated underwriting system(s) (AUS) used by the financial institution to evaluate the application) which ideally should have no impact on an application because they are not borrower statistics. Other lender choice features also include credit score type, which the borrower does not know. \n",
    "3. Data Leakage features contain information that is only available after loan approval. For example, the 'intro_rate_period' which only exists when the loan reaches the underwriting stage, or 'denial_reason_1' which only exists when applications are denied. However, these might be good for explainability reasons in post-training.\n",
    "4. Demographic features are protected features such as sex, race and age. These features are removed from the model to ensure compliance with regulations ('Equal Credit Opportunity Act') which prevent them from being used in a credit decision. However, we acknowledging that this demographic-blind approach may still leave residual bias, which is beyond the scope of this project in interpretability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f925d61a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of missing values of denial_reason_1 per action_taken:\n",
      " action_taken\n",
      "0    0.000000\n",
      "1    0.972174\n",
      "Name: denial_reason_1, dtype: float64\n",
      "We can see that denial reason almost always takes the value of 10 when the action_taken=1: loan is approved. This is data leakage.\n",
      "\n",
      "Percentage of missing values of rate_spread per action_taken:\n",
      " action_taken\n",
      "0    0.985019\n",
      "1    0.093939\n",
      "Name: rate_spread, dtype: float64\n",
      "We can see that the rate_spread is always exempt when the action_taken=0: loan is denied.\n",
      "\n",
      "Percentage of missing values of lender_credits per action_taken:\n",
      " action_taken\n",
      "0    0.984844\n",
      "1    0.752414\n",
      "Name: lender_credits, dtype: float64\n",
      "Number of NaNs: 285160\n",
      "lender_credits\n",
      "Exempt    10204\n",
      "1000.0      679\n",
      "500.0       604\n",
      "500.00      378\n",
      "50.0        346\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Percentage of missing values of purchaser_type per action_taken:\n",
      " action_taken\n",
      "0    1.000000\n",
      "1    0.436119\n",
      "Name: purchaser_type, dtype: float64\n",
      "Number of NaNs: 0\n",
      "purchaser_type\n",
      "0     202012\n",
      "71     33763\n",
      "2      27542\n",
      "1      26988\n",
      "3      26462\n",
      "Name: count, dtype: int64 \n",
      "\n",
      "Percentage of missing values of interest_rate per action_taken:\n",
      " action_taken\n",
      "0    0.984832\n",
      "1    0.003113\n",
      "Name: interest_rate, dtype: float64\n",
      "Number of NaNs: 85109\n",
      "interest_rate\n",
      "6.625     13432\n",
      "Exempt    10136\n",
      "7.125      8474\n",
      "6.875      8300\n",
      "6.125      7833\n",
      "Name: count, dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#to_process\n",
    "census_features=['tract_population', 'tract_minority_population_percent',\n",
    "       'ffiec_msa_md_median_family_income', 'tract_to_msa_income_percentage',\n",
    "       'tract_owner_occupied_units', 'tract_one_to_four_family_homes',\n",
    "       'tract_median_age_of_housing_units']\n",
    "irrelevant_feat = ['aus_1','submission_of_application', 'Unnamed: 0', 'activity_year', 'applicant_credit_score_type', 'co_applicant_credit_score_type', 'lei']\n",
    "leakage_feat=[\"denial_reason_1\", \"interest_rate\",       \n",
    "    \"total_loan_costs\",   \n",
    "    \"origination_charges\", \n",
    "    \"discount_points\",     \n",
    "    \"lender_credits\",       \n",
    "    'rate_spread', 'hoepa_status', 'purchaser_type']\n",
    "protected_feat=['derived_ethnicity', 'derived_race', 'derived_sex', 'applicant_ethnicity_1',\n",
    "       'co_applicant_ethnicity_1', 'applicant_ethnicity_observed',\n",
    "       'co_applicant_ethnicity_observed', 'applicant_race_1',\n",
    "       'co_applicant_race_1', 'applicant_race_observed',\n",
    "       'co_applicant_race_observed', 'applicant_sex', 'co_applicant_sex',\n",
    "       'applicant_sex_observed', 'co_applicant_sex_observed', 'applicant_age',\n",
    "       'co_applicant_age', 'applicant_age_above_62',\n",
    "       'co_applicant_age_above_62']\n",
    "\n",
    "\n",
    "feature='denial_reason_1' #Example of data leakage\n",
    "missing=df.groupby(target)[feature].apply(lambda x: (x==10).sum())\n",
    "total_summary = df.groupby(target)[feature].size()\n",
    "percentage=missing/total_summary\n",
    "print(f\"Percentage of missing values of {feature} per action_taken:\\n\", percentage)\n",
    "#print(\"Number of NaNs:\", df[feature].isna().sum())\n",
    "#print(df[feature].value_counts().head(5))\n",
    "print(\"We can see that denial reason almost always takes the value of 10 when the action_taken=1: loan is approved. This is data leakage.\\n\")\n",
    "\n",
    "feature='rate_spread' #Example of data leakage\n",
    "missing=df.groupby(target)[feature].apply(lambda x: x.isna().sum())\n",
    "total_summary = df.groupby(target)[feature].size()\n",
    "percentage=missing/total_summary\n",
    "print(f\"Percentage of missing values of {feature} per action_taken:\\n\", percentage)\n",
    "#print(\"Number of NaNs:\", df[feature].isna().sum())\n",
    "#print(df[feature].value_counts().head(5))\n",
    "print(\"We can see that the rate_spread is always exempt when the action_taken=0: loan is denied.\\n\")\n",
    "\n",
    "feature='lender_credits'\n",
    "missing=df.groupby(target)[feature].apply(lambda x: x.isna().sum())\n",
    "total_summary = df.groupby(target)[feature].size()\n",
    "percentage=missing/total_summary\n",
    "print(f\"Percentage of missing values of {feature} per action_taken:\\n\", percentage)\n",
    "print(\"Number of NaNs:\", df[feature].isna().sum())\n",
    "print(df[feature].value_counts().head(5), \"\\n\")\n",
    "\n",
    "\n",
    "feature='purchaser_type'\n",
    "missing=df.groupby(target)[feature].apply(lambda x: (x==0).sum())\n",
    "total_summary = df.groupby(target)[feature].size()\n",
    "percentage=missing/total_summary\n",
    "print(f\"Percentage of missing values of {feature} per action_taken:\\n\", percentage)\n",
    "print(\"Number of NaNs:\", df[feature].isna().sum())\n",
    "print(df[feature].value_counts().head(5), \"\\n\")\n",
    "\n",
    "\n",
    "feature='interest_rate'\n",
    "missing=df.groupby(target)[feature].apply(lambda x: x.isna().sum())\n",
    "total_summary = df.groupby(target)[feature].size()\n",
    "percentage=missing/total_summary\n",
    "print(f\"Percentage of missing values of {feature} per action_taken:\\n\", percentage)\n",
    "print(\"Number of NaNs:\", df[feature].isna().sum())\n",
    "print(df[feature].value_counts().head(5), \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8dd98970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are now 32 features for prediction in this cleaned dataset.\n",
      "Index(['derived_msa_md', 'state_code', 'county_code', 'census_tract',\n",
      "       'conforming_loan_limit', 'derived_loan_product_type',\n",
      "       'derived_dwelling_category', 'action_taken', 'preapproval', 'loan_type',\n",
      "       'loan_purpose', 'lien_status', 'reverse_mortgage',\n",
      "       'open_end_line_of_credit', 'business_or_commercial_purpose',\n",
      "       'loan_amount', 'combined_loan_to_value_ratio', 'loan_term',\n",
      "       'intro_rate_period', 'negative_amortization', 'interest_only_payment',\n",
      "       'balloon_payment', 'other_nonamortizing_features', 'property_value',\n",
      "       'construction_method', 'occupancy_type',\n",
      "       'manufactured_home_secured_property_type',\n",
      "       'manufactured_home_land_property_interest', 'total_units', 'income',\n",
      "       'debt_to_income_ratio', 'initially_payable_to_institution'],\n",
      "      dtype='object')\n",
      "Each feature in this list is manually examined and sorted into categories outlined in the next section before encoding. We also discovered existence of mixed data types in various columns that are dealt with via feature engineering.\n"
     ]
    }
   ],
   "source": [
    "#Dropping the above features\n",
    "todrop=census_features+irrelevant_feat+leakage_feat+protected_feat\n",
    "df_cleaned=df.drop(columns=todrop)\n",
    "df_cleaned.head(20)\n",
    "print(f\"There are now {len(df_cleaned.columns)} features for prediction in this cleaned dataset.\")\n",
    "print(df_cleaned.columns)\n",
    "print(f\"Each feature in this list is manually examined and sorted into categories outlined in the next section before encoding. We also discovered existence of mixed data types in various columns that are dealt with via feature engineering.\")\n",
    "missing_val_info={'placeholder':{},\n",
    "                  'count':{}}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f1bb4ba",
   "metadata": {},
   "source": [
    "### Note on Exempt and NaN datapoints:\n",
    "\n",
    "This dataset contains a number of values labeled as Exempt, which occur because financial institutions are permitted to withhold certain information. These exempt values can be treated as NaN for analysis purposes.\n",
    "\n",
    "Other NaN or not applicable datapoints may arise either because the information was not provided or because it does not apply to that particular loan category. For example, fixed-rate mortgages do not have an introductory rate period, so missing values in this field are expected.\n",
    "\n",
    "In some categorical variables, missing or not applicable values may also be encoded as special codes, such as 1111 or 9999. Handling of these cases is described on a case-by-case basis in the following section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e2a21",
   "metadata": {},
   "source": [
    "### Case1.1: Low Dimensional Categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "174c0880",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derived loan product type is described as a join of loan type and lien status. This graph verifies that it is fully determined by the other 2 variables, so we can drop the the derived_loan_product_type variable.\n",
      "loan_type  lien_status\n",
      "1          1              1\n",
      "           2              1\n",
      "2          1              1\n",
      "           2              1\n",
      "3          1              1\n",
      "4          1              1\n",
      "           2              1\n",
      "Name: derived_loan_product_type, dtype: int64\n",
      "\n",
      "The derived_dwelling_category is also a join from the Construction Method and Total Units fields.\n",
      "construction_method  total_units\n",
      "1                    1              1\n",
      "                     2              1\n",
      "                     3              1\n",
      "                     4              1\n",
      "                     1              1\n",
      "                     100-149        1\n",
      "                     2              1\n",
      "                     25-49          1\n",
      "                     3              1\n",
      "                     4              1\n",
      "                     5-24           1\n",
      "                     50-99          1\n",
      "                     >149           1\n",
      "2                    1              1\n",
      "                     1              1\n",
      "                     100-149        1\n",
      "                     2              1\n",
      "                     25-49          1\n",
      "                     3              1\n",
      "                     4              1\n",
      "                     5-24           1\n",
      "                     50-99          1\n",
      "                     >149           1\n",
      "Name: derived_dwelling_category, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "low_dim_cat=['occupancy_type', 'manufactured_home_land_property_interest', 'manufactured_home_secured_property_type', 'conforming_loan_limit', 'derived_loan_product_type', 'derived_dwelling_category', 'loan_type', 'loan_purpose', 'lien_status', 'initially_payable_to_institution']\n",
    "\n",
    "#ToDrop:\n",
    "todrop_redundant=[]\n",
    "counts = df_cleaned.groupby(['loan_type', 'lien_status'])['derived_loan_product_type'].nunique()\n",
    "print(\"The derived loan product type is described as a join of loan type and lien status. This graph verifies that it is fully determined by the other 2 variables, so we can drop the the derived_loan_product_type variable.\")\n",
    "print(counts)\n",
    "todrop_redundant.append('derived_loan_product_type')\n",
    "\n",
    "counts = df_cleaned.groupby(['construction_method', 'total_units'])['derived_dwelling_category'].nunique()\n",
    "print(\"\\nThe derived_dwelling_category is also a join from the Construction Method and Total Units fields.\")\n",
    "print(counts)\n",
    "todrop_redundant.append('derived_dwelling_category')\n",
    "\n",
    "df_cleaned=df_cleaned.drop(columns=todrop_redundant)\n",
    "low_dim_cat=[j for j in low_dim_cat if j not in todrop_redundant]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2e93b639",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "occupancy_type                                 0\n",
      "manufactured_home_land_property_interest       0\n",
      "manufactured_home_secured_property_type        0\n",
      "conforming_loan_limit                       1568\n",
      "loan_type                                      0\n",
      "loan_purpose                                   0\n",
      "lien_status                                    0\n",
      "initially_payable_to_institution               0\n",
      "dtype: int64 \n",
      "\n",
      "Column data type: int64\n",
      "occupancy_type\n",
      "1    317086\n",
      "3     28448\n",
      "2      7024\n",
      "Name: count, dtype: int64\n",
      "Column data type: int64\n",
      "manufactured_home_land_property_interest\n",
      "5       322694\n",
      "1        12019\n",
      "1111     10318\n",
      "3         5417\n",
      "4         1978\n",
      "2          132\n",
      "Name: count, dtype: int64\n",
      "Column data type: int64\n",
      "manufactured_home_secured_property_type\n",
      "3       322638\n",
      "2        10909\n",
      "1111     10195\n",
      "1         8816\n",
      "Name: count, dtype: int64\n",
      "Column data type: object\n",
      "conforming_loan_limit\n",
      "C     340967\n",
      "NC      9975\n",
      "U         48\n",
      "Name: count, dtype: int64\n",
      "Column data type: int64\n",
      "loan_type\n",
      "1    280407\n",
      "2     45136\n",
      "3     25191\n",
      "4      1824\n",
      "Name: count, dtype: int64\n",
      "Column data type: int64\n",
      "loan_purpose\n",
      "1     175184\n",
      "32     49382\n",
      "4      46260\n",
      "2      41701\n",
      "31     39888\n",
      "5        143\n",
      "Name: count, dtype: int64\n",
      "Column data type: int64\n",
      "lien_status\n",
      "1    255164\n",
      "2     97394\n",
      "Name: count, dtype: int64\n",
      "Column data type: int64\n",
      "initially_payable_to_institution\n",
      "1       318303\n",
      "2        17024\n",
      "1111     10156\n",
      "3         7075\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_cleaned[low_dim_cat].isna().sum(), '\\n')\n",
    "for j in low_dim_cat:\n",
    "    print(\"Column data type:\", df_cleaned[j].dtype)\n",
    "    print(df_cleaned[j].value_counts())\n",
    "    if j=='manufactured_home_land_property_interest':\n",
    "        missing_val_info['placeholder'][j]=[1111]\n",
    "    elif j=='manufactured_home_secured_property_type':\n",
    "        missing_val_info['placeholder'][j]=[1111]\n",
    "    elif j=='loan_purpose':\n",
    "        missing_val_info['placeholder'][j]=[4, 5]\n",
    "    elif j=='initially_payable_to_institution':\n",
    "        missing_val_info['placeholder'][j]=[3, 1111]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff11aa76",
   "metadata": {},
   "source": [
    "A cursory examination of the data ranges shows us that all columns are already in consistent data types. Since tabnet takes either integers or strings for categoricals. Cross-checking against data schema, we will leave the Exempt entries, encoded as 1111 in hopes tabnet can pick up these values. We will also convert the np.nan and U entries into 1111 as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e608a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column data type: int64\n",
      "occupancy_type\n",
      "1    317086\n",
      "3     28448\n",
      "2      7024\n",
      "Name: count, dtype: int64\n",
      "Column data type: int64\n",
      "manufactured_home_land_property_interest\n",
      "5       322694\n",
      "1        12019\n",
      "1111     10318\n",
      "3         5417\n",
      "4         1978\n",
      "2          132\n",
      "Name: count, dtype: int64\n",
      "Column data type: int64\n",
      "manufactured_home_secured_property_type\n",
      "3       322638\n",
      "2        10909\n",
      "1111     10195\n",
      "1         8816\n",
      "Name: count, dtype: int64\n",
      "Column data type: int64\n",
      "conforming_loan_limit\n",
      "1       340967\n",
      "0         9975\n",
      "1111      1616\n",
      "Name: count, dtype: int64\n",
      "Column data type: int64\n",
      "loan_type\n",
      "1    280407\n",
      "2     45136\n",
      "3     25191\n",
      "4      1824\n",
      "Name: count, dtype: int64\n",
      "Column data type: int64\n",
      "loan_purpose\n",
      "1     175184\n",
      "32     49382\n",
      "4      46260\n",
      "2      41701\n",
      "31     39888\n",
      "5        143\n",
      "Name: count, dtype: int64\n",
      "Column data type: int64\n",
      "lien_status\n",
      "1    255164\n",
      "2     97394\n",
      "Name: count, dtype: int64\n",
      "Column data type: int64\n",
      "initially_payable_to_institution\n",
      "1       318303\n",
      "2        17024\n",
      "1111     10156\n",
      "3         7075\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b0/ymr48tf963d9_3y2jfmn5wfw0000gn/T/ipykernel_81901/2793523652.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df_cleaned=df_cleaned.replace(replace_map, inplace=False)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "replace_map = {\n",
    "    'conforming_loan_limit': {np.nan: 1111,\n",
    "                              'C': 1,\n",
    "                              'NC': 0,\n",
    "                              'U': 1111}\n",
    "}\n",
    "missing_val_info['placeholder']['conforming_loan_limit']=[1111]\n",
    "df_cleaned=df_cleaned.replace(replace_map, inplace=False)\n",
    "for j in low_dim_cat:\n",
    "    df_cleaned[j]=df_cleaned[j].astype(int)\n",
    "    print(\"Column data type:\", df_cleaned[j].dtype)\n",
    "    print(df_cleaned[j].value_counts())\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ebdc65",
   "metadata": {},
   "source": [
    "### Case 1.2: Categoricals Features with High Dimension\n",
    "This is basic categorical data that requires usage of embeddings due to high dimensionality. The embedding is naturally dealt with by tabnet by passing as integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d07a3048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derived_msa_md\n",
      "99999    48153\n",
      "26420     6841\n",
      "38060     6601\n",
      "16984     6286\n",
      "12054     6233\n",
      "Name: count, dtype: int64\n",
      "Column data type: int64\n",
      "Number of NaNs: 0\n",
      "Number of Exempt: 0\n",
      "Number of dimensions: 418\n",
      "census_tract\n",
      "4.815767e+10    139\n",
      "4.820154e+10    111\n",
      "1.210103e+10     99\n",
      "1.209502e+10     99\n",
      "4.808503e+10     97\n",
      "Name: count, dtype: int64\n",
      "Column data type: float64\n",
      "Number of NaNs: 4876\n",
      "Number of Exempt: 0\n",
      "Number of dimensions: 74587\n",
      "county_code\n",
      "6037.0     5900\n",
      "4013.0     5723\n",
      "17031.0    4164\n",
      "48201.0    3472\n",
      "6065.0     2806\n",
      "Name: count, dtype: int64\n",
      "Column data type: float64\n",
      "Number of NaNs: 3589\n",
      "Number of Exempt: 0\n",
      "Number of dimensions: 3105\n",
      "state_code\n",
      "CA    29762\n",
      "FL    29233\n",
      "TX    28551\n",
      "NC    14477\n",
      "OH    14143\n",
      "Name: count, dtype: int64\n",
      "Column data type: object\n",
      "Number of NaNs: 1850\n",
      "Number of Exempt: 0\n",
      "Number of dimensions: 54\n"
     ]
    }
   ],
   "source": [
    "high_dim_cat=['derived_msa_md', 'census_tract', 'county_code', 'state_code']\n",
    "feature=high_dim_cat[1]\n",
    "for feature in high_dim_cat:\n",
    "    print(df_cleaned[feature].value_counts().head(5))\n",
    "    print(\"Column data type:\", df_cleaned[feature].dtype)\n",
    "    print(\"Number of NaNs:\", df_cleaned[feature].isna().sum())\n",
    "    print(\"Number of Exempt:\", (df_cleaned[feature]=='Exempt').sum())\n",
    "    print(f\"Number of dimensions: {len(df_cleaned[feature].value_counts())}\")\n",
    "\n",
    "#Drop census_tract due to high dimensionality, since it is a location variable which is too granular, and we already have other location variables\n",
    "df_cleaned=df_cleaned.drop(columns=['census_tract', 'county_code'])\n",
    "high_dim_cat.remove('census_tract')\n",
    "high_dim_cat.remove('county_code')\n",
    "df_cleaned['state_code']=df_cleaned['state_code'].fillna('Unknown')\n",
    "missing_val_info['placeholder']['state_code']=['Unknown']\n",
    "missing_val_info['placeholder']['derived_msa_md']=[99999]\n",
    "#print(min(df_cleaned['county_code'].value_counts().index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7907b86d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "['derived_msa_md', 'state_code', 'occupancy_type', 'manufactured_home_land_property_interest', 'manufactured_home_secured_property_type', 'conforming_loan_limit', 'loan_type', 'loan_purpose', 'lien_status', 'initially_payable_to_institution']\n"
     ]
    }
   ],
   "source": [
    "categoricals=high_dim_cat+low_dim_cat\n",
    "print(len(categoricals))\n",
    "print(categoricals)\n",
    "for feature in missing_val_info['placeholder'].keys():\n",
    "    missing_val_info['count'][feature]=df_cleaned[feature].isin(missing_val_info['placeholder'][feature]).sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76eaa864",
   "metadata": {},
   "source": [
    "### Case 2.1: Continuous Data\n",
    "This is basic continuous numerical data without the \"Exempt\" value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1ecb373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: income\n",
      "Data type:  float32\n",
      "Number of NaNs: 22214\n",
      "Number of Exempt: 0\n",
      "income\n",
      "0.0      4656\n",
      "60.0     3023\n",
      "75.0     2975\n",
      "100.0    2963\n",
      "80.0     2904\n",
      "90.0     2820\n",
      "70.0     2726\n",
      "65.0     2692\n",
      "120.0    2681\n",
      "72.0     2675\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Feature: loan_amount\n",
      "Data type:  float32\n",
      "Number of NaNs: 0\n",
      "Number of Exempt: 0\n",
      "loan_amount\n",
      "55000.0     17448\n",
      "105000.0    16934\n",
      "75000.0     11264\n",
      "65000.0     10842\n",
      "155000.0    10469\n",
      "35000.0     10060\n",
      "205000.0    10007\n",
      "45000.0      9840\n",
      "25000.0      9529\n",
      "85000.0      8904\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd_to_numeric=['income', 'loan_amount']\n",
    "for feature in pd_to_numeric:\n",
    "    df_cleaned[feature]=df_cleaned[feature].astype(np.float32)\n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(\"Data type: \", df_cleaned[feature].dtype)\n",
    "    print(\"Number of NaNs:\", df_cleaned[feature].isna().sum())\n",
    "    print(\"Number of Exempt:\", (df_cleaned[feature]=='Exempt').sum())\n",
    "    print(df_cleaned[feature].value_counts().head(10))\n",
    "    print(\"\\n\")\n",
    "    missing_val_info['placeholder'][feature]=[np.nan]\n",
    "    missing_val_info['count'][feature]=df_cleaned[feature].isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096a6a5a",
   "metadata": {},
   "source": [
    "### Case2.2: Continuous with Exempt:\n",
    "Continuous data with the exempt keyword. In this case, we can treat the same as nan as we gain no meaningful information from the missing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "17272f4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: intro_rate_period\n",
      "Data type:  object\n",
      "Number of NaNs: 264480\n",
      "Number of Exempt: 10149\n",
      "intro_rate_period\n",
      "1.0       33945\n",
      "1         11426\n",
      "Exempt    10149\n",
      "1.0        3382\n",
      "84.0       3072\n",
      "6.0        2928\n",
      "60.0       2856\n",
      "60         2388\n",
      "12.0       2289\n",
      "12         2257\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Feature: combined_loan_to_value_ratio\n",
      "Data type:  object\n",
      "Number of NaNs: 26931\n",
      "Number of Exempt: 10131\n",
      "combined_loan_to_value_ratio\n",
      "80.0        17164\n",
      "96.5        10787\n",
      "Exempt      10131\n",
      "100.0        7937\n",
      "95.0         7873\n",
      "75.0         6101\n",
      "90.0         5244\n",
      "80.00000     4616\n",
      "80.0         4254\n",
      "70.0         3311\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Feature: property_value\n",
      "Data type:  object\n",
      "Number of NaNs: 13105\n",
      "Number of Exempt: 10187\n",
      "property_value\n",
      "Exempt      10187\n",
      "305000.0     4431\n",
      "355000.0     4331\n",
      "255000.0     4100\n",
      "325000.0     4080\n",
      "275000.0     3873\n",
      "265000.0     3738\n",
      "295000.0     3696\n",
      "335000.0     3693\n",
      "285000.0     3690\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n",
      "Feature: loan_term\n",
      "Data type:  object\n",
      "Number of NaNs: 6634\n",
      "Number of Exempt: 10110\n",
      "loan_term\n",
      "360       120788\n",
      "360.0      80462\n",
      "360.0      32303\n",
      "300        13774\n",
      "240        12959\n",
      "180        10358\n",
      "Exempt     10110\n",
      "240.0       9108\n",
      "300.0       8175\n",
      "120         7027\n",
      "Name: count, dtype: int64\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "continuous_with_exempt=['intro_rate_period', 'combined_loan_to_value_ratio', 'property_value', 'loan_term']\n",
    "for feature in continuous_with_exempt:\n",
    "    print(f\"Feature: {feature}\")\n",
    "    print(\"Data type: \", df_cleaned[feature].dtype)\n",
    "    print(\"Number of NaNs:\", df_cleaned[feature].isna().sum())\n",
    "    print(\"Number of Exempt:\", (df_cleaned[feature]=='Exempt').sum())\n",
    "    print(df_cleaned[feature].value_counts().head(10))\n",
    "    print(\"\\n\")\n",
    "for feature in continuous_with_exempt:\n",
    "    df_cleaned[feature]=df_cleaned[feature].replace(\"Exempt\", np.nan)\n",
    "    missing_val_info['placeholder'][feature]=[np.nan]\n",
    "    missing_val_info['count'][feature]=df_cleaned[feature].isna().sum()\n",
    "    df_cleaned[feature]=df_cleaned[feature].astype(np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b782e19a",
   "metadata": {},
   "source": [
    "### Case 2.2: Continuous With Range:\n",
    "These are features that are a hybrid of numerical values and numerical ranges. When there are small ranges, we replace with midpoint. When they are extremal ranges, we replace with the extremal endpoints. Rows that already contain a direct numeric value are left unchanged, preserving the available precision.\n",
    "\n",
    "This approach is motivated by the observation that lender decisions appear to require greater granularity within certain ranges, such as the 3650 range for debt-to-income ratio. For other ranges, such as 2030 or 3036, grouping values into a single representative number is reasonable, as the decision process likely treats applicants within these ranges similarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7c3ecd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs: 28365\n",
      "Number of Exempt: 10122\n",
      "debt_to_income_ratio\n",
      "30%-<36%    43806\n",
      "20%-<30%    42901\n",
      "50%-60%     32723\n",
      ">60%        24789\n",
      "<20%        18704\n",
      "49          15678\n",
      "44          12939\n",
      "42          12642\n",
      "43          11142\n",
      "41          10899\n",
      "40          10635\n",
      "48          10443\n",
      "Exempt      10122\n",
      "39          10107\n",
      "45           9967\n",
      "47           9768\n",
      "46           9652\n",
      "38           9532\n",
      "37           8941\n",
      "36           8803\n",
      "Name: count, dtype: int64\n",
      "Number of NaNs: 0\n",
      "Number of Exempt: 0\n",
      "total_units\n",
      "1          316318\n",
      "1           25369\n",
      "2            6181\n",
      "3            1440\n",
      "5-24         1042\n",
      "4            1007\n",
      "2             513\n",
      "25-49         172\n",
      ">149          145\n",
      "50-99         137\n",
      "3              88\n",
      "4              74\n",
      "100-149        72\n",
      "Name: count, dtype: int64\n",
      "\n",
      " After: \n",
      "debt_to_income_ratio\n",
      "33.0    43806\n",
      "25.0    42901\n",
      "55.0    32723\n",
      "80.0    24789\n",
      "10.0    18704\n",
      "49.0    15678\n",
      "44.0    12939\n",
      "42.0    12642\n",
      "43.0    11142\n",
      "41.0    10899\n",
      "40.0    10635\n",
      "48.0    10443\n",
      "39.0    10107\n",
      "45.0     9967\n",
      "47.0     9768\n",
      "46.0     9652\n",
      "38.0     9532\n",
      "37.0     8941\n",
      "36.0     8803\n",
      "Name: count, dtype: int64\n",
      "total_units\n",
      "1.0      341687\n",
      "2.0        6694\n",
      "3.0        1528\n",
      "4.0        1081\n",
      "14.5       1042\n",
      "37.0        172\n",
      "200.0       145\n",
      "74.5        137\n",
      "124.5        72\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "numericwithranges_and_exempt=['debt_to_income_ratio', 'total_units']\n",
    "for feature in numericwithranges_and_exempt:\n",
    "    print(\"Number of NaNs:\", df_cleaned[feature].isna().sum())\n",
    "    print(\"Number of Exempt:\", (df_cleaned[feature]=='Exempt').sum())\n",
    "    print(df_cleaned[feature].value_counts().head(20))\n",
    "replace_map={'debt_to_income_ratio': {'30%-<36%': 33,\n",
    "                                      '20%-<30%': 25,\n",
    "                                      \"50%-60%\": 55, \n",
    "                                      '>60%': 80, \n",
    "                                      '<20%': 10, \n",
    "                                      'Exempt': np.nan},\n",
    "            'total_units': {'5-24': 29/2,\n",
    "                            '25-49': (25+49)/2,\n",
    "                            '50-99': (50+99)/2,\n",
    "                            '100-149': (100+149)/2,\n",
    "                            '>149': 200, \n",
    "                            }\n",
    "}\n",
    "#\n",
    "print(\"\\n After: \")\n",
    "df_cleaned=df_cleaned.replace(replace_map, inplace=False)\n",
    "for col in numericwithranges_and_exempt:\n",
    "    df_cleaned[col]=df_cleaned[col].astype(np.float32)\n",
    "    missing_val_info['placeholder'][feature]=[np.nan]\n",
    "    missing_val_info['count'][feature]=df_cleaned[feature].isna().sum()\n",
    "    print(df_cleaned[col].value_counts().head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e62c29d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "numeric=pd_to_numeric+numericwithranges_and_exempt+continuous_with_exempt\n",
    "for feature in missing_val_info['placeholder'].keys():\n",
    "    if missing_val_info['count'][feature]:\n",
    "        pass\n",
    "    else:\n",
    "        missing_val_info['count'][feature]=df_cleaned[feature].isin(missing_val_info['placeholder'][feature]).sum()\n",
    "print(len(numeric))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b7015d",
   "metadata": {},
   "source": [
    "### Case 3.1: Boolean Data\n",
    "Simple boolean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8217a7a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs: 0\n",
      "Number of Exempt: 0\n",
      "preapproval\n",
      "2    344525\n",
      "1      8033\n",
      "Name: count, dtype: int64\n",
      "Number of NaNs: 0\n",
      "Number of Exempt: 0\n",
      "construction_method\n",
      "1    332140\n",
      "2     20418\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "boolean=['preapproval', 'construction_method']\n",
    "for feature in boolean:\n",
    "    print(\"Number of NaNs:\", df_cleaned[feature].isna().sum())\n",
    "    print(\"Number of Exempt:\", (df_cleaned[feature]=='Exempt').sum())\n",
    "    print(df_cleaned[feature].value_counts().head(5))\n",
    "for feature in boolean:\n",
    "    df_cleaned[feature]=df_cleaned[feature].astype(int)\n",
    "    missing_val_info['placeholder'][feature]=[np.nan]\n",
    "    missing_val_info['count'][feature]=0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65847766",
   "metadata": {},
   "source": [
    "### Case 3.2: Boolean With Exempt\n",
    "The 1111 datafield is equivalent to the Exempt keyword in other columns, so can be treated as equivalent to NA. However, the entry 3 indicates that the loan is not a manufactured-home loan, so this field does not apply. There are clearly no np.nan values, all have been converted to 1111 already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1568582e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NaNs: 0\n",
      "Number of Exempt: 0\n",
      "reverse_mortgage\n",
      "2       341000\n",
      "1111     10175\n",
      "1         1383\n",
      "Name: count, dtype: int64\n",
      "Number of NaNs: 0\n",
      "Number of Exempt: 0\n",
      "open_end_line_of_credit\n",
      "2       265441\n",
      "1        76991\n",
      "1111     10126\n",
      "Name: count, dtype: int64\n",
      "Number of NaNs: 0\n",
      "Number of Exempt: 0\n",
      "business_or_commercial_purpose\n",
      "2       322472\n",
      "1        19924\n",
      "1111     10162\n",
      "Name: count, dtype: int64\n",
      "Number of NaNs: 0\n",
      "Number of Exempt: 0\n",
      "negative_amortization\n",
      "2       341661\n",
      "1111     10193\n",
      "1          704\n",
      "Name: count, dtype: int64\n",
      "Number of NaNs: 0\n",
      "Number of Exempt: 0\n",
      "interest_only_payment\n",
      "2       295735\n",
      "1        46630\n",
      "1111     10193\n",
      "Name: count, dtype: int64\n",
      "Number of NaNs: 0\n",
      "Number of Exempt: 0\n",
      "balloon_payment\n",
      "2       327609\n",
      "1        14756\n",
      "1111     10193\n",
      "Name: count, dtype: int64\n",
      "Number of NaNs: 0\n",
      "Number of Exempt: 0\n",
      "other_nonamortizing_features\n",
      "2       341356\n",
      "1111     10193\n",
      "1         1009\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "boolean_with_exempt=['reverse_mortgage', 'open_end_line_of_credit', 'business_or_commercial_purpose', 'negative_amortization', 'interest_only_payment', 'balloon_payment', 'other_nonamortizing_features']\n",
    "for feature in boolean_with_exempt:\n",
    "    print(\"Number of NaNs:\", df_cleaned[feature].isna().sum())\n",
    "    print(\"Number of Exempt:\", (df_cleaned[feature]=='Exempt').sum())\n",
    "    print(df_cleaned[feature].value_counts().head(200))\n",
    "    df_cleaned[feature]=df_cleaned[feature].astype(int)\n",
    "    missing_val_info['placeholder'][feature]=[1111]\n",
    "    missing_val_info['count'][feature]=(df_cleaned[feature]==1111).sum()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ffef5762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As a double check to make sure we have not left any features, take the set difference of columns and the set sum of feature categories: {'action_taken'}. This is the target variable as expected.\n"
     ]
    }
   ],
   "source": [
    "#categoricals=list(set(categoricals+boolean+boolean_with_exempt))\n",
    "categoricals=list(set(categoricals+boolean+boolean_with_exempt)) # need to figure out how to deal with sparsity in nnumericals\n",
    "features=categoricals\n",
    "df_cleaned=df_cleaned[features+[target]]\n",
    "print(f\"As a double check to make sure we have not left any features, take the set difference of columns and the set sum of feature categories: {set(df_cleaned.columns)-set(features)}. This is the target variable as expected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34e0a8eb",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cc8a34e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>initially_payable_to_institution</th>\n",
       "      <th>manufactured_home_land_property_interest</th>\n",
       "      <th>other_nonamortizing_features</th>\n",
       "      <th>business_or_commercial_purpose</th>\n",
       "      <th>interest_only_payment</th>\n",
       "      <th>open_end_line_of_credit</th>\n",
       "      <th>construction_method</th>\n",
       "      <th>preapproval</th>\n",
       "      <th>state_code</th>\n",
       "      <th>loan_purpose</th>\n",
       "      <th>balloon_payment</th>\n",
       "      <th>lien_status</th>\n",
       "      <th>manufactured_home_secured_property_type</th>\n",
       "      <th>derived_msa_md</th>\n",
       "      <th>reverse_mortgage</th>\n",
       "      <th>loan_type</th>\n",
       "      <th>occupancy_type</th>\n",
       "      <th>negative_amortization</th>\n",
       "      <th>conforming_loan_limit</th>\n",
       "      <th>action_taken</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>KS</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>28140</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>FL</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>45294</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>CA</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>39820</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>OH</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>99999</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>WI</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>24580</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   initially_payable_to_institution  manufactured_home_land_property_interest  \\\n",
       "0                                 1                                         5   \n",
       "1                                 1                                         5   \n",
       "2                                 1                                         5   \n",
       "3                                 1                                         5   \n",
       "4                                 1                                         5   \n",
       "\n",
       "   other_nonamortizing_features  business_or_commercial_purpose  \\\n",
       "0                             2                               2   \n",
       "1                             2                               2   \n",
       "2                             2                               2   \n",
       "3                             2                               2   \n",
       "4                             2                               2   \n",
       "\n",
       "   interest_only_payment  open_end_line_of_credit  construction_method  \\\n",
       "0                      2                        2                    1   \n",
       "1                      2                        2                    1   \n",
       "2                      2                        2                    1   \n",
       "3                      2                        2                    1   \n",
       "4                      2                        2                    1   \n",
       "\n",
       "   preapproval state_code  loan_purpose  balloon_payment  lien_status  \\\n",
       "0            2         KS            31                2            1   \n",
       "1            2         FL            32                2            1   \n",
       "2            2         CA             2                2            2   \n",
       "3            2         OH             1                2            1   \n",
       "4            2         WI             1                2            1   \n",
       "\n",
       "   manufactured_home_secured_property_type  derived_msa_md  reverse_mortgage  \\\n",
       "0                                        3           28140                 2   \n",
       "1                                        3           45294                 2   \n",
       "2                                        3           39820                 2   \n",
       "3                                        3           99999                 2   \n",
       "4                                        3           24580                 2   \n",
       "\n",
       "   loan_type  occupancy_type  negative_amortization  conforming_loan_limit  \\\n",
       "0          1               1                      2                      1   \n",
       "1          1               1                      2                      1   \n",
       "2          1               1                      2                      1   \n",
       "3          2               1                      2                      1   \n",
       "4          1               1                      2                      1   \n",
       "\n",
       "   action_taken  \n",
       "0             1  \n",
       "1             1  \n",
       "2             0  \n",
       "3             1  \n",
       "4             0  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cleaned = df_cleaned.reset_index(drop=True)\n",
    "df_cleaned.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "aa55a014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4, 6, 3, 3, 3, 3, 2, 2, 55, 6, 3, 2, 4, 418, 3, 4, 3, 3, 3]\n"
     ]
    }
   ],
   "source": [
    "#Encode Categoricals\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "categorical_columns=categoricals\n",
    "categorical_dims={col: len(df_cleaned[col].value_counts()) for col in categorical_columns}\n",
    "cat_idxs = [ i for i, f in enumerate(features) if f in categorical_columns]\n",
    "cat_dims = [ categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n",
    "j=[]\n",
    "for col in categorical_columns:\n",
    "    l_enc = LabelEncoder()\n",
    "    df_cleaned[col]=l_enc.fit_transform(df_cleaned[col].values)\n",
    "print(cat_dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d1906e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_features=None\n",
    "#grouped_features = [[0, 1, 2], [8, 9, 10]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a64e0c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed_value):\n",
    "    random.seed(seed_value)\n",
    "    np.random.seed(seed_value)\n",
    "    torch.manual_seed(seed_value)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed_value)\n",
    "    \n",
    "    if torch.cuda.is_available(): \n",
    "        torch.cuda.manual_seed(seed_value)\n",
    "        torch.cuda.manual_seed_all(seed_value)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "seed=10\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df249f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " initially_payable_to_institution: all good\n",
      " manufactured_home_land_property_interest: all good\n",
      " other_nonamortizing_features: all good\n",
      " business_or_commercial_purpose: all good\n",
      " interest_only_payment: all good\n",
      " open_end_line_of_credit: all good\n",
      " construction_method: all good\n",
      " preapproval: all good\n",
      " state_code: all good\n",
      " loan_purpose: all good\n",
      " balloon_payment: all good\n",
      " lien_status: all good\n",
      " manufactured_home_secured_property_type: all good\n",
      " derived_msa_md: all good\n",
      " reverse_mortgage: all good\n",
      " loan_type: all good\n",
      " occupancy_type: all good\n",
      " negative_amortization: all good\n",
      " conforming_loan_limit: all good\n",
      "Length of X_train: 281820\n",
      "Length of X_train: 35710\n",
      "Length of X_train: 35028\n"
     ]
    }
   ],
   "source": [
    "#Train Test Split\n",
    "if \"trainingset\" not in df_cleaned.columns:\n",
    "    df_cleaned[\"trainingset\"] = np.random.choice([\"train\", \"valid\", \"test\"], p =[.8, .1, .1], size=(df_cleaned.shape[0],))\n",
    "train_indices = df_cleaned[df_cleaned['trainingset']==\"train\"].index\n",
    "valid_indices = df_cleaned[df_cleaned['trainingset']==\"valid\"].index\n",
    "test_indices = df_cleaned[df_cleaned['trainingset']==\"test\"].index\n",
    "df_cleaned=df_cleaned.drop(columns=['trainingset'])\n",
    "X_train = df_cleaned[features].values[train_indices]\n",
    "y_train = df_cleaned[target].values[train_indices]\n",
    "\n",
    "X_valid = df_cleaned[features].values[valid_indices]\n",
    "y_valid = df_cleaned[target].values[valid_indices]\n",
    "\n",
    "X_test = df_cleaned[features].values[test_indices]\n",
    "y_test = df_cleaned[target].values[test_indices]\n",
    "\n",
    "for col in categorical_columns:\n",
    "    train_vals = set(df_cleaned.loc[train_indices, col].dropna().unique())\n",
    "    test_vals  = set(df_cleaned.loc[test_indices,  col].dropna().unique())\n",
    "\n",
    "    unseen = test_vals - train_vals\n",
    "    if unseen:\n",
    "        print(f\"Unseen categories in test for {col}: {unseen}\")\n",
    "    else:\n",
    "        print(f\" {col}: all good\")\n",
    "\n",
    "print(f\"Length of X_train: {len(X_train)}\")\n",
    "print(f\"Length of X_train: {len(X_valid)}\")\n",
    "print(f\"Length of X_train: {len(X_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "528fa974",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24272885596128863, 0.7572711440387114]\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "neg=len(df_cleaned[df_cleaned['action_taken']==0])/len(df_cleaned)\n",
    "weights=[neg, 1-neg]\n",
    "print(weights)\n",
    "class_weights = torch.tensor([1/weights[0], 1/weights[1]])\n",
    "def Weighted_CE(input, target):\n",
    "    return F.cross_entropy(input, target, weight=class_weights)\n",
    "\n",
    "def Weighted_CE_per_batch(input, target):\n",
    "    \"\"\"\n",
    "    input: logits [batch_size, 2]\n",
    "    target: class indices [batch_size]\n",
    "    \"\"\"\n",
    "    batch_size = target.size(0)\n",
    "    num_class0 = (target == 0).sum().item()\n",
    "    num_class1 = (target == 1).sum().item()\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    num_class0 = max(num_class0, 1)\n",
    "    num_class1 = max(num_class1, 1)\n",
    "    \n",
    "    # Class weights inversely proportional to frequency in this batch\n",
    "    weight = torch.tensor([batch_size/num_class0, batch_size/num_class1], dtype=torch.float, device=input.device)\n",
    "    \n",
    "    # Compute CE with these per-batch weights\n",
    "    return F.cross_entropy(input, target, weight=weight, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0a53fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/Desktop/TrustWorthyXAILocaltoGlobal/utilities/models/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0  | loss: 0.59571 | valid_auc: 0.7435  | valid_accuracy: 0.66712 | valid_balanced_accuracy: 0.70651 | valid_logloss: 0.57194 |  0:00:35s\n",
      "[0.16765536 0.04843124 0.0149674  0.0071017  0.00180372 0.05684715\n",
      " 0.05012785 0.15425318 0.02062409 0.23589909 0.010447   0.07953157\n",
      " 0.02486263 0.00310487 0.00741317 0.05733326 0.01860522 0.03426244\n",
      " 0.00672907]\n",
      "epoch 1  | loss: 0.56791 | valid_auc: 0.77234 | valid_accuracy: 0.66655 | valid_balanced_accuracy: 0.70963 | valid_logloss: 0.56159 |  0:01:37s\n",
      "[1.43242759e-01 3.04814040e-02 1.97470580e-02 4.45417435e-02\n",
      " 5.45676096e-04 4.67898353e-02 4.24058903e-02 1.16646353e-01\n",
      " 5.31350411e-02 3.11395051e-01 4.13515860e-03 5.82448256e-02\n",
      " 5.59451003e-02 9.10135617e-03 4.53713480e-03 4.25365136e-02\n",
      " 6.34811789e-03 1.00450702e-02 1.75912283e-04]\n",
      "epoch 2  | loss: 0.56718 | valid_auc: 0.77232 | valid_accuracy: 0.66351 | valid_balanced_accuracy: 0.70822 | valid_logloss: 0.55566 |  0:02:37s\n",
      "[1.47330544e-01 1.42969163e-02 3.02587314e-03 1.78456584e-02\n",
      " 3.07420838e-03 5.03665894e-02 1.97895122e-02 2.17809680e-01\n",
      " 5.77853041e-02 3.48718747e-01 1.93311148e-04 3.16017501e-02\n",
      " 2.26044878e-02 9.23727337e-03 7.93875579e-03 1.20888582e-02\n",
      " 6.89741957e-04 3.52174493e-02 3.85339220e-04]\n",
      "epoch 3  | loss: 0.56628 | valid_auc: 0.77732 | valid_accuracy: 0.67238 | valid_balanced_accuracy: 0.70903 | valid_logloss: 0.55183 |  0:03:38s\n",
      "[1.12102201e-01 5.29482328e-04 2.70309791e-03 1.07606894e-02\n",
      " 4.28478785e-04 7.25509317e-02 5.80853551e-02 1.87513549e-01\n",
      " 3.35493113e-02 3.47370077e-01 9.48494021e-05 9.40799222e-02\n",
      " 1.08012816e-02 1.53063297e-05 1.41438758e-05 1.01330833e-02\n",
      " 5.65675419e-05 5.92001118e-02 1.15604296e-05]\n",
      "epoch 4  | loss: 0.56099 | valid_auc: 0.77978 | valid_accuracy: 0.67329 | valid_balanced_accuracy: 0.71043 | valid_logloss: 0.56811 |  0:04:36s\n",
      "[7.15049189e-02 2.08763714e-05 2.23665823e-02 4.07542653e-02\n",
      " 1.90786874e-04 8.37864734e-02 3.56199009e-02 1.51095714e-01\n",
      " 2.62426407e-02 3.74037626e-01 1.74573404e-05 1.10804249e-01\n",
      " 1.06087795e-02 2.42461839e-05 2.07212234e-03 1.22488206e-02\n",
      " 6.45015281e-04 5.43885047e-02 3.57102028e-03]\n",
      "Stop training because you reached max_epochs = 5 with best_epoch = 3 and best_valid_logloss = 0.55183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alex/Desktop/TrustWorthyXAILocaltoGlobal/utilities/models/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully saved model at test_model.zip\n"
     ]
    }
   ],
   "source": [
    "from utilities.models.pytorch_tabnet.tab_model import TabNetClassifier\n",
    "tabnet_classifier_params={\n",
    "                'n_d': 32,\n",
    "                'n_a': 32,\n",
    "                'n_steps': 5,\n",
    "                'gamma': 1.6,\n",
    "                'lambda_sparse': 5e-3,\n",
    "                'momentum': 0.08,\n",
    "                \"cat_idxs\":cat_idxs,\n",
    "                \"cat_dims\":cat_dims,\n",
    "                \"cat_emb_dim\":[min(40, int(j**0.5)) for j in cat_dims],\n",
    "                \"optimizer_fn\":torch.optim.Adam,\n",
    "                \"optimizer_params\":dict(lr=2e-2, weight_decay=1e-5),\n",
    "                \"scheduler_params\":dict(\n",
    "                                        mode=\"min\",\n",
    "                                        factor=0.5,\n",
    "                                        patience=5,\n",
    "                                        min_lr=1e-6,\n",
    "                                                    ),\n",
    "                \"scheduler_fn\":torch.optim.lr_scheduler.\n",
    "                \"mask_type\":'entmax', # \"sparsemax\"\n",
    "                #\"grouped_features\" : \n",
    "                }\n",
    "\n",
    "model=TabNetClassifier(**tabnet_classifier_params)\n",
    "\n",
    "model.fit_with_epochFI(\n",
    "    X_train=X_train, y_train=y_train,\n",
    "    eval_set=[(X_valid, y_valid)],\n",
    "    eval_name=['valid'],\n",
    "    eval_metric=['auc', 'accuracy', 'balanced_accuracy', 'logloss'],\n",
    "    max_epochs=5 ,\n",
    "    patience=10, # please be patient ^^\n",
    "    loss_fn=Weighted_CE_per_batch,\n",
    "    batch_size=512,\n",
    "    virtual_batch_size=256,\n",
    "    num_workers=0,\n",
    "    drop_last=False,\n",
    "    compute_importance=True\n",
    ")\n",
    "saved=model.save_model('test_model')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e7f9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Loss\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(model.history['loss'], label='Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 2 AUC\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(model.history['valid_auc'], label='Valid AUC')\n",
    "plt.title('AUC over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 3 Accuracy\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(model.history['valid_accuracy'], label='Valid Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Feature Importance Drift\n",
    "fi = np.array(model.feature_importances_history)\n",
    "num_epochs, num_features = fi.shape\n",
    "epochs = np.arange(1, num_epochs + 1)\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "for feature_idx in range(num_features):\n",
    "    plt.plot(epochs, fi[:, feature_idx], label=f\"Feature {features[feature_idx]}\")\n",
    "\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Feature Importance\")\n",
    "plt.title(\"TabNet Feature Importances Over Training\")\n",
    "plt.legend(ncol=2, fontsize=8)\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22510bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prediction:\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_pred=model.predict(X_test)\n",
    "test_acc = accuracy_score(y_pred=y_pred, y_true=y_test)\n",
    "print(f\"FINAL TEST SCORE FOR {filename} : {test_acc}\")\n",
    "\n",
    "#load by doing model.load_model(saved)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e23843e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Explanations\n",
    "importances=model.feature_importances_\n",
    "feature_expl_stats=pd.DataFrame({'feature': features,\n",
    "                                 'importance': model.feature_importances_,\n",
    "                                 'Missing/Exempt percentage': [missing_val_info['count'][feature] if feature in missing_val_info['count'] else 0 for feature in features]\n",
    "})\n",
    "feature_expl_stats=feature_expl_stats.sort_values(by='importance', ascending=False)\n",
    "print(feature_expl_stats)\n",
    "print(f\"Total number of parameters: {sum(p.numel() for p in model.network.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51cfecc1",
   "metadata": {},
   "source": [
    "## Influence Function Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27ef7c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from utilities.influence.calc_influence_function import calc_influence_batch, compute_influence_for_test_point, calc_average_influence_of_point\n",
    "from utilities.influence.utils import init_logging, get_default_config\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import datetime\n",
    "import logging\n",
    "# Wrapper class to make TabNet compatible with influence function\n",
    "class TabNetWrapper(nn.Module):\n",
    "    \"\"\"Wrapper to extract logits from TabNet's tuple output\"\"\"\n",
    "    def __init__(self, tabnet_network):\n",
    "        super().__init__()\n",
    "        self.tabnet_network = tabnet_network\n",
    "    \n",
    "    def forward(self, x):\n",
    "        output = self.tabnet_network(x)\n",
    "        # TabNet returns (output, mask) tuple, extract just the logits\n",
    "        if isinstance(output, tuple) and len(output)==2:\n",
    "            return output[0]  # Return logits (first element)\n",
    "        return output  # If not a tuple, return as-is\n",
    "\n",
    "# model = TabNetClassifier()\n",
    "# model.load_model('test_model.zip')\n",
    "torch_model = TabNetWrapper(model.network)\n",
    "torch_model.eval()\n",
    "\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
    "valid_dataset = TensorDataset(torch.tensor(X_valid, dtype=torch.float32), torch.tensor(y_valid, dtype=torch.long))\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=512, shuffle=True, num_workers=0)\n",
    "test_loader = DataLoader(test_dataset, batch_size=512, shuffle=True, num_workers=0)\n",
    "\n",
    "# Configure and run influence calculation\n",
    "init_logging()\n",
    "config = {\n",
    "        'outdir': 'outdir',\n",
    "        'seed': 42,\n",
    "        'device': 0 if torch.cuda.is_available() else -1,\n",
    "        'dataset': '2024LAR',\n",
    "        'num_classes': 2,\n",
    "        'test_sample_num': False,\n",
    "        'test_start_index': False,\n",
    "        'recursion_depth': 500, #Paper suggests recursion_depth * r =datasetsize\n",
    "        'r_averaging': 3,\n",
    "        'scale': 25,\n",
    "        'damp': 0.01,\n",
    "        'log_filename': None,\n",
    "    }\n",
    "\n",
    "influences=[]\n",
    "harmful=[]\n",
    "helpful=[]\n",
    "print(X_train[:, cat_idxs].dtype)\n",
    "print(X_test[:, cat_idxs].dtype)\n",
    "\n",
    "logging.disable(logging.DEBUG)\n",
    "for index in range(50):\n",
    "    influence_i, harmful_i, helpful_i=compute_influence_for_test_point(model=torch_model, train_loader=train_loader, test_loader=test_loader, test_index=index, config=config, time_logging=True)\n",
    "    influences.append(influence_i)\n",
    "    harmful.append(harmful_i)\n",
    "    helpful.append(helpful_i)\n",
    "#i benchmarked it, took 6.6 hours on cluster to calculate all influences for fixed z_test, but the actual pairwise is quite fast for influence(z_test, z_train). so i think we should j sample.\n",
    "#took 69 seconds for influence of (test, train) pair.\n",
    "print(f\"Number of Helpful Datatpoints: {len(helpful)}\")\n",
    "print(f\"Number of Harmful Datatpoints: {len(harmful)}\")\n",
    "\n",
    "#calculate global influence of a single training point\n",
    "training_indices = random.sample(range(1, len(X_train)), 30)\n",
    "avg_infs=[]\n",
    "test_indices=random.sample(range(1, len(X_test)), 500)\n",
    "for index in training_indices:\n",
    "    time_a=datetime.datetime.now()\n",
    "    average_inf=calc_average_influence_of_point(model=torch_model, train_loader=train_loader, test_loader=test_loader, train_index=index, config=config, test_indices=test_indices)\n",
    "    time_b=datetime.datetime.now()\n",
    "    time_delta=time_b-time_a\n",
    "    avg_infs.append((index, average_inf, time_delta.total_seconds()))\n",
    "    print(f\"The Global average influence of point {37} is {average_inf}. Took {time_delta.total_seconds()} seconds to compute\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5e5dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Influence Results\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_influence_results(influence_results, test_id=0, top_k=10, show_train_data=False):\n",
    "    \"\"\"\n",
    "    Visualize influence results for a specific test sample.\n",
    "    \n",
    "    Args:\n",
    "        influence_results: Dictionary returned by calc_influence_dataset\n",
    "        test_id: Index of test sample to visualize\n",
    "        top_k: Number of top helpful/harmful training samples to show\n",
    "        show_train_data: If True, show actual feature values of training samples\n",
    "    \"\"\"\n",
    "    if str(test_id) not in influence_results:\n",
    "        print(f\"Test ID {test_id} not found in results. Available IDs: {list(influence_results.keys())[:10]}\")\n",
    "        return\n",
    "    \n",
    "    result = influence_results[str(test_id)]\n",
    "    influences = result['influence']\n",
    "    harmful_indices = result['harmful'][:top_k]\n",
    "    helpful_indices = result['helpful'][:top_k]\n",
    "    \n",
    "    # Create DataFrame for easier manipulation\n",
    "    df = pd.DataFrame({\n",
    "        'train_idx': range(len(influences)),\n",
    "        'influence': influences\n",
    "    })\n",
    "    \n",
    "    # Get top helpful and harmful\n",
    "    helpful_df = df.loc[helpful_indices].copy()\n",
    "    helpful_df = helpful_df.sort_values('influence', ascending=False)\n",
    "    \n",
    "    harmful_df = df.loc[harmful_indices].copy()\n",
    "    harmful_df = harmful_df.sort_values('influence', ascending=True)\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Plot harmful influences\n",
    "    axes[0].barh(range(len(harmful_df)), harmful_df['influence'].values, color='coral', alpha=0.7)\n",
    "    axes[0].set_yticks(range(len(harmful_df)))\n",
    "    axes[0].set_yticklabels([f\"Train {idx}\" for idx in harmful_df['train_idx'].values])\n",
    "    axes[0].set_xlabel('Influence Score (negative = harmful)', fontsize=11)\n",
    "    axes[0].set_title(f'Top {top_k} Most Harmful Training Samples\\n(Test Sample {test_id}, Label: {result[\"label\"]})', fontsize=12, fontweight='bold')\n",
    "    axes[0].grid(axis='x', alpha=0.3)\n",
    "    axes[0].invert_yaxis()\n",
    "    \n",
    "    # Plot helpful influences\n",
    "    axes[1].barh(range(len(helpful_df)), helpful_df['influence'].values, color='lightgreen', alpha=0.7)\n",
    "    axes[1].set_yticks(range(len(helpful_df)))\n",
    "    axes[1].set_yticklabels([f\"Train {idx}\" for idx in helpful_df['train_idx'].values])\n",
    "    axes[1].set_xlabel('Influence Score (positive = helpful)', fontsize=11)\n",
    "    axes[1].set_title(f'Top {top_k} Most Helpful Training Samples\\n(Test Sample {test_id}, Label: {result[\"label\"]})', fontsize=12, fontweight='bold')\n",
    "    axes[1].grid(axis='x', alpha=0.3)\n",
    "    axes[1].invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary statistics\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Influence Analysis Summary for Test Sample {test_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Test Label: {result['label']}\")\n",
    "    print(f\"Calculation Time: {result['time_calc_influence_s']:.2f} seconds\")\n",
    "    print(f\"\\nInfluence Statistics:\")\n",
    "    print(f\"  Mean: {np.mean(influences):.6f}\")\n",
    "    print(f\"  Std:  {np.std(influences):.6f}\")\n",
    "    print(f\"  Min:  {np.min(influences):.6f}\")\n",
    "    print(f\"  Max:  {np.max(influences):.6f}\")\n",
    "    print(f\"\\nMost Harmful Training Sample: {harmful_indices[0]} (influence: {influences[harmful_indices[0]]:.6f})\")\n",
    "    print(f\"Most Helpful Training Sample: {helpful_indices[0]} (influence: {influences[helpful_indices[0]]:.6f})\")\n",
    "    \n",
    "    # Optionally show training data for top samples\n",
    "    if show_train_data and 'X_train' in globals():\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Top Helpful Training Sample Features:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        top_helpful_idx = helpful_indices[0]\n",
    "        top_helpful_features = pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'value': X_train[top_helpful_idx]\n",
    "        })\n",
    "        print(top_helpful_features.to_string(index=False))\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Top Harmful Training Sample Features:\")\n",
    "        print(f\"{'='*60}\")\n",
    "        top_harmful_idx = harmful_indices[0]\n",
    "        top_harmful_features = pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'value': X_train[top_harmful_idx]\n",
    "        })\n",
    "        print(top_harmful_features.to_string(index=False))\n",
    "\n",
    "# Visualize results for first test sample\n",
    "if 'influence_results' in globals():\n",
    "    visualize_influence_results(influence_results, test_id=0, top_k=10, show_train_data=True)\n",
    "else:\n",
    "    print(\"Run the influence calculation cell first\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb871a8d",
   "metadata": {},
   "source": [
    "## Local Rule Extraction using Influence Functions\n",
    "\n",
    "This section extracts human-readable local decision rules for TabNet predictions using influence functions. The pipeline:\n",
    "\n",
    "1. **Identify Influential Training Samples**: Uses influence functions to find training points that support (S_plus) or oppose (S_minus) the prediction\n",
    "2. **Cluster Influential Samples**: Groups influential samples by features, model predictions, and influence sign\n",
    "3. **Derive Symbolic Rules**: Extracts decision tree rules from each cluster to explain \"why\" and \"what if\" scenarios\n",
    "\n",
    "These local rules are designed to be aggregated later into global rules (GLocalX-style).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7f420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local Rule Extraction Pipeline\n",
    "from utilities.influence.local_rule_extraction import LocalRuleExtractor\n",
    "\n",
    "# Initialize rule extractor\n",
    "if 'torch_model' in globals() and 'features' in globals() and 'X_train' in globals() and 'y_train' in globals():\n",
    "    rule_extractor = LocalRuleExtractor(\n",
    "        model=torch_model,\n",
    "        feature_names=features,\n",
    "        n_clusters=3,  # Number of clusters for influential samples\n",
    "        max_depth=4,   # Max depth for decision trees (keep shallow for interpretability)\n",
    "        min_samples_split=10  # Minimum samples to split a node\n",
    "    )\n",
    "    print(\"Rule extractor initialized successfully!\")\n",
    "    print(f\"Features: {len(features)}\")\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "else:\n",
    "    print(\"Error: Required variables (torch_model, features, X_train, y_train) not found.\")\n",
    "    print(\"Please run the model training and influence calculation cells first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38971f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Local Rules for a Test Sample\n",
    "\n",
    "if 'influence_results' in globals() and 'rule_extractor' in globals():\n",
    "    # Extract rules for the first test sample (test_id=0 in influence_results)\n",
    "    # Note: The test_id in influence_results corresponds to indices in test_loader\n",
    "    # We need to map this to actual test set indices\n",
    "    \n",
    "    test_id = 0  # Change this to analyze different test samples\n",
    "    \n",
    "    print(f\"Extracting local rules for test sample {test_id}...\")\n",
    "    print(\"This may take a few minutes...\")\n",
    "    \n",
    "    # Extract rules\n",
    "    # Note: test_id in influence_results refers to test set indices\n",
    "    local_rules = rule_extractor.extract_local_rules(\n",
    "        influence_results=influence_results,\n",
    "        test_id=test_id,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        X_test=X_test if 'X_test' in globals() else None,  # Provide test set if available\n",
    "        top_k=50,  # Number of top helpful/harmful samples to consider\n",
    "        clustering_method='kmeans'  # or 'dbscan'\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Local Rules Extracted for Test Sample {test_id}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"Test Prediction: {local_rules['test_prediction']}\")\n",
    "    print(f\"Number of Helpful Samples: {len(local_rules['helpful_samples'])}\")\n",
    "    print(f\"Number of Harmful Samples: {len(local_rules['harmful_samples'])}\")\n",
    "    print(f\"Number of Clusters: {local_rules['n_clusters']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"Error: influence_results or rule_extractor not found.\")\n",
    "    print(\"Please run the influence calculation and rule extractor initialization cells first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e24c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize and Display Extracted Rules\n",
    "\n",
    "def display_local_rules(local_rules, feature_names):\n",
    "    \"\"\"Display extracted local rules in a human-readable format.\"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"LOCAL EXPLANATION RULES FOR TEST SAMPLE {local_rules['test_id']}\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    print(f\"Model Prediction: Class {local_rules['test_prediction']}\\n\")\n",
    "    \n",
    "    for cluster_id, cluster_info in local_rules['clusters'].items():\n",
    "        if cluster_info['n_samples'] < 5:  # Skip very small clusters\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\n{''*70}\")\n",
    "        print(f\"CLUSTER {cluster_id}\")\n",
    "        print(f\"{''*70}\")\n",
    "        print(f\"  Samples: {cluster_info['n_samples']}\")\n",
    "        print(f\"  Dominant Label: {cluster_info['dominant_label']}\")\n",
    "        print(f\"  Mean Influence: {cluster_info['mean_influence']:.6f}\")\n",
    "        \n",
    "        if cluster_info['explanation_rule']:\n",
    "            print(f\"\\n   EXPLANATION RULE (Why this prediction?):\")\n",
    "            print(f\"     {cluster_info['explanation_rule']}\")\n",
    "        \n",
    "        if cluster_info['counterfactual_rule']:\n",
    "            print(f\"\\n   COUNTERFACTUAL RULE (What needs to change?):\")\n",
    "            print(f\"     {cluster_info['counterfactual_rule']}\")\n",
    "    \n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(\"Note: These local rules can be aggregated across many instances\")\n",
    "    print(\"to discover global patterns (future: GLocalX-style aggregation)\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "# Display rules if available\n",
    "if 'local_rules' in globals():\n",
    "    display_local_rules(local_rules, features)\n",
    "else:\n",
    "    print(\"No local rules found. Please run the rule extraction cell first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "879fdc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Cluster Structure and Rule Coverage\n",
    "\n",
    "def visualize_rule_clusters(local_rules, X_train, feature_names, top_features=5):\n",
    "    \"\"\"Visualize how clusters relate to features and influences.\"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    \n",
    "    if 'local_rules' not in globals():\n",
    "        print(\"No local rules to visualize.\")\n",
    "        return\n",
    "    \n",
    "    # Get cluster data\n",
    "    cluster_labels = np.array(local_rules['cluster_data']['cluster_labels'])\n",
    "    influence_signs = np.array(local_rules['cluster_data']['influence_signs'])\n",
    "    \n",
    "    # Get features for influential samples\n",
    "    all_indices = local_rules['helpful_samples'] + local_rules['harmful_samples']\n",
    "    influential_features = X_train[all_indices]\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Cluster distribution by influence sign\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    cluster_influence_counts = []\n",
    "    cluster_labels_list = []\n",
    "    influence_type = []\n",
    "    \n",
    "    for cluster_id in unique_clusters:\n",
    "        if cluster_id == -1:  # Skip noise\n",
    "            continue\n",
    "        mask = cluster_labels == cluster_id\n",
    "        helpful_count = (influence_signs[mask] > 0).sum()\n",
    "        harmful_count = (influence_signs[mask] < 0).sum()\n",
    "        \n",
    "        cluster_influence_counts.extend([helpful_count, harmful_count])\n",
    "        cluster_labels_list.extend([f'Cluster {cluster_id}', f'Cluster {cluster_id}'])\n",
    "        influence_type.extend(['Helpful', 'Harmful'])\n",
    "    \n",
    "    if cluster_influence_counts:\n",
    "        df_cluster = pd.DataFrame({\n",
    "            'Cluster': cluster_labels_list,\n",
    "            'Count': cluster_influence_counts,\n",
    "            'Type': influence_type\n",
    "        })\n",
    "        sns.barplot(data=df_cluster, x='Cluster', y='Count', hue='Type', ax=axes[0, 0])\n",
    "        axes[0, 0].set_title('Cluster Distribution by Influence Type')\n",
    "        axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # 2. Feature importance across clusters (using variance)\n",
    "    feature_variance = np.var(influential_features, axis=0)\n",
    "    top_feature_indices = np.argsort(feature_variance)[-top_features:][::-1]\n",
    "    top_feature_names = [feature_names[i] for i in top_feature_indices]\n",
    "    \n",
    "    axes[0, 1].barh(range(len(top_feature_names)), feature_variance[top_feature_indices])\n",
    "    axes[0, 1].set_yticks(range(len(top_feature_names)))\n",
    "    axes[0, 1].set_yticklabels(top_feature_names)\n",
    "    axes[0, 1].set_xlabel('Feature Variance')\n",
    "    axes[0, 1].set_title(f'Top {top_features} Most Variable Features in Influential Samples')\n",
    "    axes[0, 1].invert_yaxis()\n",
    "    \n",
    "    # 3. Cluster separation (2D projection using PCA on top features)\n",
    "    from sklearn.decomposition import PCA\n",
    "    \n",
    "    if len(influential_features) > 1:\n",
    "        pca = PCA(n_components=2)\n",
    "        features_2d = pca.fit_transform(influential_features[:, top_feature_indices])\n",
    "        \n",
    "        scatter = axes[1, 0].scatter(features_2d[:, 0], features_2d[:, 1], \n",
    "                                    c=cluster_labels, cmap='tab10', \n",
    "                                    alpha=0.6, s=50)\n",
    "        axes[1, 0].set_xlabel(f'PC1 (explains {pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "        axes[1, 0].set_ylabel(f'PC2 (explains {pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "        axes[1, 0].set_title('Cluster Visualization (2D Projection)')\n",
    "        axes[1, 0].legend(*scatter.legend_elements(), title=\"Clusters\")\n",
    "        plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    \n",
    "    # 4. Rule coverage: Show which clusters have valid rules\n",
    "    cluster_ids = []\n",
    "    has_explanation = []\n",
    "    has_counterfactual = []\n",
    "    n_samples_list = []\n",
    "    \n",
    "    for cluster_id, cluster_info in local_rules['clusters'].items():\n",
    "        cluster_ids.append(f'Cluster {cluster_id}')\n",
    "        has_explanation.append(1 if cluster_info['explanation_rule'] else 0)\n",
    "        has_counterfactual.append(1 if cluster_info['counterfactual_rule'] else 0)\n",
    "        n_samples_list.append(cluster_info['n_samples'])\n",
    "    \n",
    "    if cluster_ids:\n",
    "        x = np.arange(len(cluster_ids))\n",
    "        width = 0.35\n",
    "        axes[1, 1].bar(x - width/2, has_explanation, width, label='Has Explanation Rule', color='lightblue')\n",
    "        axes[1, 1].bar(x + width/2, has_counterfactual, width, label='Has Counterfactual Rule', color='lightcoral')\n",
    "        axes[1, 1].set_xlabel('Cluster')\n",
    "        axes[1, 1].set_ylabel('Rule Availability')\n",
    "        axes[1, 1].set_title('Rule Coverage by Cluster')\n",
    "        axes[1, 1].set_xticks(x)\n",
    "        axes[1, 1].set_xticklabels(cluster_ids, rotation=45, ha='right')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].set_ylim([0, 1.2])\n",
    "        \n",
    "        # Add sample counts as text\n",
    "        for i, n_samples in enumerate(n_samples_list):\n",
    "            axes[1, 1].text(i, 1.1, f'n={n_samples}', ha='center', fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nSummary:\")\n",
    "    print(f\"  Total clusters: {local_rules['n_clusters']}\")\n",
    "    print(f\"  Clusters with explanation rules: {sum(1 for c in local_rules['clusters'].values() if c['explanation_rule'])}\")\n",
    "    print(f\"  Clusters with counterfactual rules: {sum(1 for c in local_rules['clusters'].values() if c['counterfactual_rule'])}\")\n",
    "\n",
    "# Visualize if rules are available\n",
    "if 'local_rules' in globals():\n",
    "    visualize_rule_clusters(local_rules, X_train, features, top_features=10)\n",
    "else:\n",
    "    print(\"No local rules to visualize. Please run the rule extraction cell first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a70966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional Visualization: Compare Multiple Test Samples or Load Saved Results\n",
    "\n",
    "# Option 1: Load previously saved results\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "def load_saved_influence_results(results_path='outdir/influence_results.json'):\n",
    "    \"\"\"Load influence results from a saved JSON file.\"\"\"\n",
    "    path = Path(results_path)\n",
    "    if path.exists():\n",
    "        with open(path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    else:\n",
    "        print(f\"Results file not found at {results_path}\")\n",
    "        return None\n",
    "\n",
    "# Uncomment to load saved results:\n",
    "# influence_results = load_saved_influence_results()\n",
    "\n",
    "# Option 2: Compare influences across multiple test samples\n",
    "def compare_test_samples(influence_results, test_ids=[0, 1, 2], top_k=5):\n",
    "    \"\"\"Compare top helpful/harmful training samples across multiple test samples.\"\"\"\n",
    "    fig, axes = plt.subplots(len(test_ids), 2, figsize=(14, 4*len(test_ids)))\n",
    "    if len(test_ids) == 1:\n",
    "        axes = axes.reshape(1, -1)\n",
    "    \n",
    "    for idx, test_id in enumerate(test_ids):\n",
    "        if str(test_id) not in influence_results:\n",
    "            print(f\"Skipping test ID {test_id} (not found)\")\n",
    "            continue\n",
    "            \n",
    "        result = influence_results[str(test_id)]\n",
    "        influences = result['influence']\n",
    "        harmful_indices = result['harmful'][:top_k]\n",
    "        helpful_indices = result['helpful'][:top_k]\n",
    "        \n",
    "        harmful_infl = [influences[i] for i in harmful_indices]\n",
    "        helpful_infl = [influences[i] for i in helpful_indices]\n",
    "        \n",
    "        # Plot harmful\n",
    "        axes[idx, 0].barh(range(len(harmful_infl)), harmful_infl, color='coral', alpha=0.7)\n",
    "        axes[idx, 0].set_yticks(range(len(harmful_indices)))\n",
    "        axes[idx, 0].set_yticklabels([f\"T{i}\" for i in harmful_indices])\n",
    "        axes[idx, 0].set_xlabel('Influence')\n",
    "        axes[idx, 0].set_title(f'Test {test_id} (Label: {result[\"label\"]}) - Harmful')\n",
    "        axes[idx, 0].invert_yaxis()\n",
    "        axes[idx, 0].grid(axis='x', alpha=0.3)\n",
    "        \n",
    "        # Plot helpful\n",
    "        axes[idx, 1].barh(range(len(helpful_infl)), helpful_infl, color='lightgreen', alpha=0.7)\n",
    "        axes[idx, 1].set_yticks(range(len(helpful_indices)))\n",
    "        axes[idx, 1].set_yticklabels([f\"T{i}\" for i in helpful_indices])\n",
    "        axes[idx, 1].set_xlabel('Influence')\n",
    "        axes[idx, 1].set_title(f'Test {test_id} (Label: {result[\"label\"]}) - Helpful')\n",
    "        axes[idx, 1].invert_yaxis()\n",
    "        axes[idx, 1].grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Option 3: Distribution of influence scores\n",
    "def plot_influence_distribution(influence_results, test_id=0):\n",
    "    \"\"\"Plot the distribution of influence scores for a test sample.\"\"\"\n",
    "    if str(test_id) not in influence_results:\n",
    "        print(f\"Test ID {test_id} not found\")\n",
    "        return\n",
    "    \n",
    "    influences = influence_results[str(test_id)]['influence']\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Histogram\n",
    "    axes[0].hist(influences, bins=50, edgecolor='black', alpha=0.7)\n",
    "    axes[0].axvline(0, color='red', linestyle='--', linewidth=2, label='Zero influence')\n",
    "    axes[0].set_xlabel('Influence Score')\n",
    "    axes[0].set_ylabel('Frequency')\n",
    "    axes[0].set_title(f'Distribution of Influence Scores\\n(Test Sample {test_id})')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # Box plot\n",
    "    axes[1].boxplot(influences, vert=True)\n",
    "    axes[1].set_ylabel('Influence Score')\n",
    "    axes[1].set_title(f'Box Plot of Influence Scores\\n(Test Sample {test_id})')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nInfluence Distribution Statistics for Test Sample {test_id}:\")\n",
    "    print(f\"  Median: {np.median(influences):.6f}\")\n",
    "    print(f\"  Q1 (25th percentile): {np.percentile(influences, 25):.6f}\")\n",
    "    print(f\"  Q3 (75th percentile): {np.percentile(influences, 75):.6f}\")\n",
    "    print(f\"  IQR: {np.percentile(influences, 75) - np.percentile(influences, 25):.6f}\")\n",
    "\n",
    "\n",
    "if 'influence_results' in globals():\n",
    "    # Compare multiple test samples\n",
    "    compare_test_samples(influence_results, test_ids=[0, 1, 2], top_k=5)\n",
    "    \n",
    "    # Plot distribution for first test sample\n",
    "    plot_influence_distribution(influence_results, test_id=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eee0c25",
   "metadata": {},
   "source": [
    "## Global Rule Set from Local Rule Set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
